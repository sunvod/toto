{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Running AutoTheta Model on BOOM Benckmark\n",
    "Adapted from original naive.ipynb notebook from [the GiftEval repo](https://github.com/SalesforceAIResearch/gift-eval/tree/main/notebooks)\n",
    "\n",
    "This notebook shows how to run the AutoTheta model on the boom benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download BOOM datasets. Calling `download_boom_benchmark` also sets the `BOOM` environment variable with the correct path, which is needed for running the evals below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-25T19:51:31.593626Z",
     "iopub.status.busy": "2025-04-25T19:51:31.593265Z",
     "iopub.status.idle": "2025-04-25T19:51:32.548248Z",
     "shell.execute_reply": "2025-04-25T19:51:32.547650Z",
     "shell.execute_reply.started": "2025-04-25T19:51:31.593598Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from dataset_utils import download_boom_benchmark\n",
    "\n",
    "boom_path = \"ChangeMe\"\n",
    "download_boom_benchmark(boom_path)\n",
    "load_dotenv()\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))\n",
    "all_datasets = list(dataset_properties_map.keys())\n",
    "print(len(all_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-25T19:51:36.946130Z",
     "iopub.status.busy": "2025-04-25T19:51:36.945450Z",
     "iopub.status.idle": "2025-04-25T19:51:36.982432Z",
     "shell.execute_reply": "2025-04-25T19:51:36.981916Z",
     "shell.execute_reply.started": "2025-04-25T19:51:36.946096Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## StatsForecast Predictor\n",
    "\n",
    "We will use the `StatsForecastPredictor` class to wrap the AutoTheta model. This class is a wrapper around the `StatsForecast` library and is used to make it compatible with the `gluonts` interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-25T19:51:39.930639Z",
     "iopub.status.busy": "2025-04-25T19:51:39.930300Z",
     "iopub.status.idle": "2025-04-25T19:51:41.167136Z",
     "shell.execute_reply": "2025-04-25T19:51:41.166536Z",
     "shell.execute_reply.started": "2025-04-25T19:51:39.930614Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Iterator, List, Optional, Type\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gluonts.core.component import validated\n",
    "from gluonts.dataset import Dataset\n",
    "from gluonts.dataset.util import forecast_start\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "from gluonts.model.predictor import RepresentablePredictor\n",
    "from gluonts.transform.feature import LastValueImputation, MissingValueImputation\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    Naive,\n",
    "    SeasonalNaive,\n",
    "    AutoTheta,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    quantile_levels: Optional[List[float]] = None\n",
    "    forecast_keys: List[str] = field(init=False)\n",
    "    statsforecast_keys: List[str] = field(init=False)\n",
    "    intervals: Optional[List[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.forecast_keys = [\"mean\"]\n",
    "        self.statsforecast_keys = [\"mean\"]\n",
    "        if self.quantile_levels is None:\n",
    "            self.intervals = None\n",
    "            return\n",
    "\n",
    "        intervals = set()\n",
    "\n",
    "        for quantile_level in self.quantile_levels:\n",
    "            interval = round(200 * (max(quantile_level, 1 - quantile_level) - 0.5))\n",
    "            intervals.add(interval)\n",
    "            side = \"hi\" if quantile_level > 0.5 else \"lo\"\n",
    "            self.forecast_keys.append(str(quantile_level))\n",
    "            self.statsforecast_keys.append(f\"{side}-{interval}\")\n",
    "\n",
    "        self.intervals = sorted(intervals)\n",
    "\n",
    "\n",
    "class StatsForecastPredictor(RepresentablePredictor):\n",
    "    \"\"\"\n",
    "    A predictor type that wraps models from the `statsforecast`_ package.\n",
    "\n",
    "    This class is used via subclassing and setting the ``ModelType`` class\n",
    "    attribute to specify the ``statsforecast`` model type to use.\n",
    "\n",
    "    .. _statsforecast: https://github.com/Nixtla/statsforecast\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction_length\n",
    "        Prediction length for the model to use.\n",
    "    quantile_levels\n",
    "        Optional list of quantile levels that we want predictions for.\n",
    "        Note: this is only supported by specific types of models, such as\n",
    "        ``AutoARIMA``. By default this is ``None``, giving only the mean\n",
    "        prediction.\n",
    "    **model_params\n",
    "        Keyword arguments to be passed to the model type for construction.\n",
    "        The specific arguments accepted or required depend on the\n",
    "        ``ModelType``; please refer to the documentation of ``statsforecast``\n",
    "        for details.\n",
    "    \"\"\"\n",
    "\n",
    "    ModelType: Type\n",
    "\n",
    "    @validated()\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_length: int,\n",
    "        season_length: int,\n",
    "        freq: str,\n",
    "        quantile_levels: Optional[List[float]] = None,\n",
    "        imputation_method: MissingValueImputation = LastValueImputation(),\n",
    "        max_length: Optional[int] = None,\n",
    "        batch_size: int = 1,\n",
    "        parallel: bool = False,\n",
    "        **model_params,\n",
    "    ) -> None:\n",
    "        super().__init__(prediction_length=prediction_length)\n",
    "\n",
    "        if \"season_length\" in inspect.signature(self.ModelType.__init__).parameters:\n",
    "            model_params[\"season_length\"] = season_length\n",
    "\n",
    "        self.freq = freq\n",
    "        self.model = StatsForecast(\n",
    "            models=[self.ModelType(**model_params)],\n",
    "            freq=freq,\n",
    "            fallback_model=SeasonalNaive(season_length=season_length),\n",
    "            n_jobs=20 if parallel else 1,\n",
    "        )\n",
    "        self.fallback_model = StatsForecast(\n",
    "            # Fallback model when main model returns NaNs\n",
    "            models=[SeasonalNaive(season_length=season_length)],\n",
    "            freq=freq,\n",
    "            n_jobs=12 if parallel else 1,\n",
    "        )\n",
    "        self.config = ModelConfig(quantile_levels=quantile_levels)\n",
    "        self.imputation_method = imputation_method\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        if max_length == None:\n",
    "            self.max_length = 1000\n",
    "\n",
    "        # Set up the logger\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    def predict(self, dataset: Dataset, **kwargs) -> Iterator[Forecast]:\n",
    "        batch = {}\n",
    "        total_series = len(dataset)\n",
    "        self.logger.info(f\"Starting prediction on {total_series} series.\")\n",
    "\n",
    "        for idx, entry in enumerate(dataset):\n",
    "            assert entry[\"target\"].ndim == 1, \"only for univariate time series\"\n",
    "            assert len(entry[\"target\"]) >= 1, \"all time series should have at least one data point\"\n",
    "\n",
    "            if self.max_length is not None:\n",
    "                entry[\"start\"] += len(entry[\"target\"][: -self.max_length])\n",
    "                entry[\"target\"] = entry[\"target\"][-self.max_length :]\n",
    "\n",
    "            target = np.asarray(entry[\"target\"], np.float32)\n",
    "            if np.isnan(target).any():\n",
    "                target = target.copy()\n",
    "                target = self.imputation_method(target)\n",
    "\n",
    "            unique_id = f\"{entry['item_id']}_{str(forecast_start(entry))}_{str(len(batch))}\"\n",
    "            start = entry[\"start\"]\n",
    "            batch[unique_id] = pd.DataFrame(\n",
    "                {\n",
    "                    \"unique_id\": unique_id,\n",
    "                    \"ds\": pd.date_range(\n",
    "                        start=start.to_timestamp(),\n",
    "                        periods=len(target),\n",
    "                        freq=start.freq,\n",
    "                    ).to_numpy(),\n",
    "                    \"y\": target,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if len(batch) == self.batch_size:\n",
    "                self.logger.info(f\"Processing batch {idx // self.batch_size + 1}.\")\n",
    "                results = self.sf_predict(pd.concat(batch.values()))\n",
    "                yield from self.yield_forecast(batch.keys(), results)\n",
    "                batch = {}\n",
    "\n",
    "        if len(batch) > 0:\n",
    "            self.logger.info(f\"Processing final batch.\")\n",
    "            results = self.sf_predict(pd.concat(batch.values()))\n",
    "            yield from self.yield_forecast(batch.keys(), results)\n",
    "\n",
    "        self.logger.info(\"Prediction completed.\")\n",
    "\n",
    "    def sf_predict(self, Y_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        kwargs = {}\n",
    "        if self.config.intervals is not None:\n",
    "            kwargs[\"level\"] = self.config.intervals\n",
    "        results = self.model.forecast(\n",
    "            df=Y_df,\n",
    "            h=self.prediction_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # replace nan results with fallback\n",
    "        row_nan = results.isnull().values.any(axis=-1)\n",
    "        if row_nan.any():\n",
    "            nan_ids = results[row_nan].index.values\n",
    "            nan_df = Y_df[Y_df[\"unique_id\"].isin(nan_ids)]\n",
    "            fallback_results = self.fallback_model.forecast(\n",
    "                df=nan_df,\n",
    "                h=self.prediction_length,\n",
    "                **kwargs,\n",
    "            )\n",
    "            results = pd.concat(\n",
    "                [\n",
    "                    results[~results.index.isin(nan_ids)],\n",
    "                    fallback_results,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def yield_forecast(self, item_ids, results: pd.DataFrame) -> Iterator[QuantileForecast]:\n",
    "        results.set_index(\"unique_id\", inplace=True)\n",
    "        for idx in item_ids:\n",
    "            prediction = results.loc[idx]\n",
    "            forecast_arrays = []\n",
    "            model_name = self.ModelType.__name__\n",
    "            for key in self.config.statsforecast_keys:\n",
    "                if key == \"mean\":\n",
    "                    forecast_arrays.append(prediction.loc[:, model_name].to_numpy())\n",
    "                else:\n",
    "                    forecast_arrays.append(prediction.loc[:, f\"{model_name}-{key}\"].to_numpy())\n",
    "\n",
    "            yield QuantileForecast(\n",
    "                forecast_arrays=np.stack(forecast_arrays, axis=0),\n",
    "                forecast_keys=self.config.forecast_keys,\n",
    "                start_date=prediction.ds.iloc[0].to_period(freq=self.freq),\n",
    "                item_id=idx,\n",
    "            )\n",
    "\n",
    "\n",
    "class AutoThetaPredictor(StatsForecastPredictor):\n",
    "    ModelType = AutoTheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the boom benchmark datasets. We will use the `evaluate_model` function to evaluate the model. We are going to store the results in a csv file called `all_results.csv` under the `results/autotheta` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n",
    "Note that for datasets that cause trouble for autoarima searching algorithm. We set a timer for each dataset and replace it with baseline methods whenever the dataset times out or causes an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-25T19:52:37.038391Z",
     "iopub.status.busy": "2025-04-25T19:52:37.037783Z"
    },
    "frozen": false,
    "tags": [
     "naive_eval",
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from gluonts.model import evaluate_model\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from gift_eval.data import Dataset\n",
    "import torch\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Iterate over all available datasets\n",
    "model_name = \"autotheta\"\n",
    "output_dir = f\"ChangeMe/{model_name}\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "            \"dataset_size\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    # if ds_num != 1623:\n",
    "    #     continue\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    dataset_term = dataset_properties_map[ds_name][\"term\"]\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or term == \"long\") and dataset_term == \"short\":\n",
    "            continue\n",
    "\n",
    "        ds_freq = dataset_properties_map[ds_name][\"frequency\"]\n",
    "        ds_config = f\"{ds_name}/{ds_freq}/{term}\"\n",
    "\n",
    "        # Initialize the dataset\n",
    "        to_univariate = False if Dataset(name=ds_name, term=term,to_univariate=False,storage_env_var=\"BOOM\").target_dim == 1 else True\n",
    "\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate,storage_env_var=\"BOOM\")\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        dataset_size = len(dataset.test_data)\n",
    "        print(f\"Dataset size: {dataset_size}\")\n",
    "        # Initialize the predictor\n",
    "        predictor = AutoThetaPredictor(\n",
    "            dataset.prediction_length,\n",
    "            season_length=season_length,\n",
    "            freq=dataset.freq,\n",
    "            quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "            batch_size=512,\n",
    "            parallel=True\n",
    "        )\n",
    "        import multiprocessing\n",
    "        def safe_evaluate_model(queue):\n",
    "            try:\n",
    "                res = evaluate_model(\n",
    "                    predictor,\n",
    "                    test_data=dataset.test_data,\n",
    "                    metrics=metrics,\n",
    "                    batch_size=512,\n",
    "                    axis=None,\n",
    "                    mask_invalid_label=True,\n",
    "                    allow_nan_forecast=False,\n",
    "                    seasonality=season_length,\n",
    "                )\n",
    "                queue.put(res)\n",
    "            except Exception as e:\n",
    "                queue.put(e)\n",
    "        \n",
    "        # Setup\n",
    "        queue = multiprocessing.Queue()\n",
    "        process = multiprocessing.Process(target=safe_evaluate_model, args=(queue,))\n",
    "        process.start()\n",
    "        process.join(timeout=45)  # timeout in seconds\n",
    "        \n",
    "        # Handle result\n",
    "        if process.is_alive():\n",
    "            print(f\"replacing results of {ds_name} with seasonal naive scores due to time out\")\n",
    "            process.terminate()\n",
    "            process.join()\n",
    "            res = pd.read_csv(f\"ChangeMe/seasonalnaive/all_results.csv\")\n",
    "            prefix = \"eval_metrics/\"\n",
    "            res.columns = [col[len(prefix):] if col.startswith(prefix) else col for col in res.columns]\n",
    "            res = res[res[\"dataset\"]==ds_config]\n",
    "            res = res.reset_index(drop=True)\n",
    "        elif not queue.empty():\n",
    "            result = queue.get()\n",
    "            if isinstance(result, Exception):\n",
    "                raise result\n",
    "                res = None\n",
    "            else:\n",
    "                res = result\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    \"autotheta\",\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_name][\"domain\"],\n",
    "                    dataset_properties_map[ds_name][\"num_variates\"],\n",
    "                    dataset_size,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/autotheta` folder containing the results for the AutoTheta model on the boom benchmark. The csv file will look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-14T18:31:55.084714Z",
     "iopub.status.busy": "2025-04-14T18:31:55.084345Z",
     "iopub.status.idle": "2025-04-14T18:31:55.210833Z",
     "shell.execute_reply": "2025-04-14T18:31:55.210309Z",
     "shell.execute_reply.started": "2025-04-14T18:31:55.084685Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_dir + \"/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "dd-sharing": {
   "allowed_groups": [
    "subproduct-datascience",
    "combined-data-science",
    "team-largemodelfoundationsresearch",
    ""
   ],
   "allowed_users": [
    ""
   ],
   "retention_period": "90"
  },
  "kernelspec": {
   "display_name": "baseline_eval_env",
   "language": "python",
   "name": "baseline_eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
