{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "# Quick Start: Running Chronos and Chronos-Bolt models on BOOM benchmark\n",
    "\n",
    "This notebook is adapted from the [GiftEval repository](https://github.com/SalesforceAIResearch/gift-eval/tree/main/notebooks) and shows how to run Chronos and Chronos-Bolt models on the BOOM benchmark.\n",
    "\n",
    "Make sure you download the BOOM benchmark and set the `BOOM` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class from GiftEval to load the data and run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download BOOM datasets. Calling `download_boom_benchmark` also sets the `BOOM` environment variable with the correct path, which is needed for running the evals below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-24T14:42:32.575787Z",
     "iopub.status.busy": "2025-04-24T14:42:32.575209Z",
     "iopub.status.idle": "2025-04-24T14:42:32.605627Z",
     "shell.execute_reply": "2025-04-24T14:42:32.605113Z",
     "shell.execute_reply.started": "2025-04-24T14:42:32.575761Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from dataset_utils import download_boom_benchmark\n",
    "\n",
    "boom_path = \"ChangeMe\"\n",
    "download_boom_benchmark(boom_path)\n",
    "load_dotenv()\n",
    "\n",
    "dataset_properties_map = json.load(open(\"eval/boom/dataset_properties.json\"))\n",
    "all_datasets = list(dataset_properties_map.keys())\n",
    "print(len(all_datasets))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the metrics that we want to compute for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-24T14:42:55.121738Z",
     "iopub.status.busy": "2025-04-24T14:42:55.121393Z",
     "iopub.status.idle": "2025-04-24T14:42:55.158588Z",
     "shell.execute_reply": "2025-04-24T14:42:55.158044Z",
     "shell.execute_reply.started": "2025-04-24T14:42:55.121715Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "## Chronos Predictor\n",
    "\n",
    "For foundation models, we need to implement a wrapper containing the model and use the wrapper to generate predicitons.\n",
    "\n",
    "This is just meant to be a simple wrapper to get you started, feel free to use your own custom implementation to wrap any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-24T14:42:59.308984Z",
     "iopub.status.busy": "2025-04-24T14:42:59.308701Z",
     "iopub.status.idle": "2025-04-24T14:43:07.483773Z",
     "shell.execute_reply": "2025-04-24T14:43:07.483112Z",
     "shell.execute_reply.started": "2025-04-24T14:42:59.308962Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from chronos import BaseChronosPipeline, ForecastType\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    quantile_levels: Optional[List[float]] = None\n",
    "    forecast_keys: List[str] = field(init=False)\n",
    "    statsforecast_keys: List[str] = field(init=False)\n",
    "    intervals: Optional[List[int]] = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.forecast_keys = [\"mean\"]\n",
    "        self.statsforecast_keys = [\"mean\"]\n",
    "        if self.quantile_levels is None:\n",
    "            self.intervals = None\n",
    "            return\n",
    "\n",
    "        intervals = set()\n",
    "\n",
    "        for quantile_level in self.quantile_levels:\n",
    "            interval = round(200 * (max(quantile_level, 1 - quantile_level) - 0.5))\n",
    "            intervals.add(interval)\n",
    "            side = \"hi\" if quantile_level > 0.5 else \"lo\"\n",
    "            self.forecast_keys.append(str(quantile_level))\n",
    "            self.statsforecast_keys.append(f\"{side}-{interval}\")\n",
    "\n",
    "        self.intervals = sorted(intervals)\n",
    "\n",
    "\n",
    "class ChronosPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        num_samples: int,\n",
    "        prediction_length: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        print(\"prediction_length:\", prediction_length)\n",
    "        self.pipeline = BaseChronosPipeline.from_pretrained(\n",
    "            model_path,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.prediction_length = prediction_length\n",
    "        self.num_samples = num_samples\n",
    "        print(\"forecast type:\", self.pipeline.forecast_type)\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "        pipeline = self.pipeline\n",
    "        predict_kwargs = {\"num_samples\": self.num_samples} if pipeline.forecast_type == ForecastType.SAMPLES else {}\n",
    "        while True:\n",
    "            try:\n",
    "                # Generate forecast samples\n",
    "                forecast_outputs = []\n",
    "                for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "                    context = [torch.tensor(entry[\"target\"]) for entry in batch]\n",
    "                    forecast_outputs.append(\n",
    "                        pipeline.predict(\n",
    "                            context,\n",
    "                            prediction_length=self.prediction_length,\n",
    "                            **predict_kwargs,\n",
    "                        ).numpy()\n",
    "                    )\n",
    "                forecast_outputs = np.concatenate(forecast_outputs)\n",
    "                break\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(f\"OutOfMemoryError at batch_size {batch_size}, reducing to {batch_size // 2}\")\n",
    "                batch_size //= 2\n",
    "\n",
    "        # Convert forecast samples into gluonts Forecast objects\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "\n",
    "            if pipeline.forecast_type == ForecastType.SAMPLES:\n",
    "                forecasts.append(SampleForecast(samples=item, start_date=forecast_start_date))\n",
    "            elif pipeline.forecast_type == ForecastType.QUANTILES:\n",
    "                forecasts.append(\n",
    "                    QuantileForecast(\n",
    "                        forecast_arrays=item,\n",
    "                        forecast_keys=list(map(str, pipeline.quantiles)),\n",
    "                        start_date=forecast_start_date,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the BOOM datasets. We will use the `evaluate_model` function from `gluonts` to evaluate the model. Then we store the results in a csv file called `all_results.csv` under the `results/{model name}` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-24T14:43:07.485156Z",
     "iopub.status.busy": "2025-04-24T14:43:07.484721Z",
     "iopub.status.idle": "2025-04-24T14:43:07.523874Z",
     "shell.execute_reply": "2025-04-24T14:43:07.523383Z",
     "shell.execute_reply.started": "2025-04-24T14:43:07.485134Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(WarningFilter(\"The mean prediction is not stored in the forecast data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-24T14:44:48.018682Z",
     "iopub.status.busy": "2025-04-24T14:44:48.018232Z",
     "iopub.status.idle": "2025-04-24T14:44:59.451177Z",
     "shell.execute_reply": "2025-04-24T14:44:59.450604Z",
     "shell.execute_reply.started": "2025-04-24T14:44:48.018655Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "# Iterate over all available datasets\n",
    "\n",
    "model_name = \"chronos_bolt_base\"\n",
    "output_dir = f\"ChangeMe/{model_name}\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "            \"dataset_size\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    dataset_term = dataset_properties_map[ds_name][\"term\"]\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or term == \"long\") and dataset_term == \"short\":\n",
    "            continue\n",
    "\n",
    "        ds_freq = dataset_properties_map[ds_name][\"frequency\"]\n",
    "        ds_config = f\"{ds_name}/{ds_freq}/{term}\"\n",
    "\n",
    "        # Initialize the dataset\n",
    "        to_univariate = False if Dataset(name=ds_name, term=term,to_univariate=False,storage_env_var=\"BOOM\").target_dim == 1 else True\n",
    "\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate,storage_env_var=\"BOOM\")\n",
    "\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        dataset_size = len(dataset.test_data)\n",
    "        print(f\"Dataset size: {dataset_size}\")\n",
    "        predictor = ChronosPredictor(\n",
    "            # use \"amazon/chronos-t5-base\" for the corresponding original Chronos model\n",
    "            model_path=\"amazon/chronos-bolt-base\",\n",
    "            num_samples=20,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "            device_map=\"cuda:0\",\n",
    "        )\n",
    "        \n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=1024,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    model_name,\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_name][\"domain\"],\n",
    "                    dataset_properties_map[ds_name][\"num_variates\"],\n",
    "                    dataset_size,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/chronos` folder containing the results for the Chronos model on the boom benchmark. We can display the csv file using the follow code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-15T23:16:11.066854Z",
     "iopub.status.busy": "2025-04-15T23:16:11.065827Z",
     "iopub.status.idle": "2025-04-15T23:16:11.169549Z",
     "shell.execute_reply": "2025-04-15T23:16:11.168841Z",
     "shell.execute_reply.started": "2025-04-15T23:16:11.066808Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_dir + \"/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "dd-sharing": {
   "allowed_groups": [
    "subproduct-datascience",
    "combined-data-science",
    "team-largemodelfoundationsresearch",
    ""
   ],
   "allowed_users": [
    ""
   ],
   "retention_period": "90"
  },
  "kernelspec": {
   "display_name": "chronos_eval_env",
   "language": "python",
   "name": "chronos_eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
