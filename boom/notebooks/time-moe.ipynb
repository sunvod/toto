{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15485824",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "# Running Time-MOE models on BOOM benchmark\n",
    "\n",
    "This notebook is adapted from the [GiftEval repository](https://github.com/SalesforceAIResearch/gift-eval/tree/main/notebooks) and shows how to run the Time-MOE models on the BOOM benchmark.\n",
    "\n",
    "Make sure you download the BOOM benchmark and set the `BOOM` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class from GiftEval to load the data and run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0da53e",
   "metadata": {},
   "source": [
    "Download BOOM datasets. Calling `download_boom_benchmark` also sets the `BOOM` environment variable with the correct path, which is needed for running the evals below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f602b74-50ad-445a-a0c2-ec34dd98ceae",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T20:56:04.242981Z",
     "iopub.status.busy": "2025-02-18T20:56:04.242535Z",
     "iopub.status.idle": "2025-02-18T20:56:10.708407Z",
     "shell.execute_reply": "2025-02-18T20:56:10.707304Z",
     "shell.execute_reply.started": "2025-02-18T20:56:04.242961Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import importlib.util\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda.amp\n",
    "\n",
    "\n",
    "\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "\n",
    "from typing import List, Union, Tuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gluonts.itertools import batcher\n",
    "from transformers import AutoModelForCausalLM\n",
    "from dataset_utils import download_boom_benchmark\n",
    "\n",
    "boom_path = \"ChangeMe\"\n",
    "download_boom_benchmark(boom_path)\n",
    "load_dotenv()\n",
    "\n",
    "dataset_properties_map = json.load(open(\"./boom/boom_properties.json\"))\n",
    "all_datasets = list(dataset_properties_map.keys())\n",
    "print(len(all_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a95a4c-f8b2-4245-a72a-09d524a12996",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T20:56:16.135201Z",
     "iopub.status.busy": "2025-02-18T20:56:16.134809Z",
     "iopub.status.idle": "2025-02-18T20:56:16.165831Z",
     "shell.execute_reply": "2025-02-18T20:56:16.165201Z",
     "shell.execute_reply.started": "2025-02-18T20:56:16.135181Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393edc0-a375-431b-ae93-1d327161f693",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T20:56:27.402501Z",
     "iopub.status.busy": "2025-02-18T20:56:27.402210Z",
     "iopub.status.idle": "2025-02-18T20:56:27.443238Z",
     "shell.execute_reply": "2025-02-18T20:56:27.442287Z",
     "shell.execute_reply.started": "2025-02-18T20:56:27.402483Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "class timemoe_wrapper:\n",
    "    def __init__(self, prediction_length, truncate = 2000, batch_size = 32):\n",
    "        model_name = \"Maple728/TimeMoE-50M\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map = self.device, trust_remote_code=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.truncate = truncate\n",
    "        self.prediction_length = prediction_length\n",
    "    def calculate_max_shape(self, values: List[Union[torch.Tensor, np.ndarray]]) -> Tuple[int, ...]:\n",
    "            \"\"\"Calculate the maximum shape for a list of tensors or arrays.\"\"\"\n",
    "            return tuple(\n",
    "                max(v.size(dim) if isinstance(v, torch.Tensor) else v.shape[dim] for v in values)\n",
    "                for dim in range(len(values[0].shape))\n",
    "            )\n",
    "\n",
    "    def pad_and_stack(self, values: List[Union[torch.Tensor, np.ndarray]], max_shape: Tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"Pad and stack tensors or arrays to the given max shape.\"\"\"\n",
    "        padded_values: List[torch.Tensor] = [\n",
    "            (\n",
    "                F.pad(\n",
    "                    v,\n",
    "                    [\n",
    "                        value\n",
    "                        for dim, max_dim in enumerate(reversed(max_shape))\n",
    "                        for value in (0, max_dim - (v.size(dim) if isinstance(v, torch.Tensor) else v.shape[dim]))\n",
    "                    ],\n",
    "                )\n",
    "                if isinstance(v, torch.Tensor)\n",
    "                else torch.tensor(\n",
    "                    np.pad(\n",
    "                        v, [(0, max_dim - v.shape[dim]) for dim, max_dim in enumerate(max_shape)], mode=\"constant\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for v in values\n",
    "        ]\n",
    "        return torch.stack(padded_values)\n",
    "    \n",
    "    def truncate_and_stack(self, tensor_list, maxlength):\n",
    "        \"\"\"\n",
    "        Truncates each tensor to maxlength. If a tensor is shorter than maxlength, it is left-padded with zeros.\n",
    "        Uses torch.jit.fork to parallelize the extraction and padding process.\n",
    "        \n",
    "        Args:\n",
    "            tensor_list (list of torch.Tensor): List of tensors with shape (length) or (num_channel, length).\n",
    "            maxlength (int): The fixed length to truncate or pad tensors.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Stacked tensor with shape (N, maxlength) for 1D tensors or (N, num_channel, maxlength) for 2D tensors.\n",
    "        \"\"\"\n",
    "        def process_tensor(t):\n",
    "            # t = self.normalize_input(t)\n",
    "            if t.dim() == 1:\n",
    "                pad_size = maxlength - t.shape[0]\n",
    "                if pad_size > 0:\n",
    "                    t = torch.cat([torch.zeros(pad_size, dtype=t.dtype, device=t.device), t])\n",
    "                return t[-maxlength:]\n",
    "            else:\n",
    "                pad_size = maxlength - t.shape[1]\n",
    "                if pad_size > 0:\n",
    "                    t = torch.cat([torch.zeros(t.shape[0], pad_size, dtype=t.dtype, device=t.device), t], dim=1)\n",
    "                return t[:, -maxlength:]\n",
    "        \n",
    "        futures = [torch.jit.fork(process_tensor, t) for t in tensor_list]  # Parallel extraction and padding\n",
    "        extracted_tensors = [torch.jit.wait(f) for f in futures]  # Wait for all to complete\n",
    "        return torch.stack(extracted_tensors, dim=0)  # Stack along first dimension\n",
    "    def normalize_input(self, inputs):\n",
    "        mean, std = inputs.mean(dim=-1, keepdim=True), inputs.std(dim=-1, keepdim=True)\n",
    "        normed_input = (inputs - mean) / std\n",
    "        return normed_input, mean, std\n",
    "    def predict(self, data):\n",
    "        self.model.eval()\n",
    "        self.model = self.model.to(self.device)\n",
    "        while self.batch_size>=1:\n",
    "            try:\n",
    "                print(\"Trying batch size\", self.batch_size)\n",
    "                forecasts = []\n",
    "                process_inputs = []\n",
    "                outputs = []\n",
    "                for batch in batcher(data, batch_size=self.batch_size):\n",
    "                    inputs = [torch.tensor(entry[\"target\"]) for entry in batch]\n",
    "                    if not self.truncate:\n",
    "                        max_shape = self.calculate_max_shape(inputs)\n",
    "                        padded_input = self.pad_and_stack(inputs, max_shape)\n",
    "                    else:\n",
    "                        padded_input = self.truncate_and_stack(inputs, self.truncate)\n",
    "                    if len(padded_input.shape)>2:\n",
    "                        padded_input = padded_input.squeeze(1)\n",
    "                    padded_input = padded_input.to(self.device).float()\n",
    "                    padded_input, means, stds = self.normalize_input(padded_input)\n",
    "                    # print(padded_input.shape)\n",
    "                    with torch.no_grad():\n",
    "                        output = self.model.generate(padded_input, max_new_tokens=self.prediction_length)\n",
    "                        output = output*stds+means\n",
    "                    forecasts.append(output[:, -self.prediction_length:].cpu().numpy())\n",
    "                    process_inputs.append(padded_input.cpu().numpy())\n",
    "                    outputs.append(output.cpu().numpy())\n",
    "                forecasts = np.concatenate(forecasts)\n",
    "                process_inputs = np.concatenate(process_inputs)\n",
    "                outputs = np.concatenate(outputs)\n",
    "                break\n",
    "            except RuntimeError:\n",
    "                self.batch_size = self.batch_size//2\n",
    "                print(f\"Batch size too large, reducing to {self.batch_size}\")\n",
    "        if self.batch_size < 1:\n",
    "            raise ValueError(\"Batch size too small\")\n",
    "        \n",
    "        forecasts_date = []\n",
    "        for item, ts in zip(forecasts, data):\n",
    "            item = item[np.newaxis,:]\n",
    "            # print(item.shape)\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "\n",
    "            forecasts_date.append(SampleForecast(samples=item, start_date=forecast_start_date))\n",
    "            \n",
    "\n",
    "        # return forecasts_date, process_inputs, forecasts, outputs\n",
    "        return forecasts_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2090b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the boom benchmark datasets. We will use the `evaluate_model` function from `gluonts` to evaluate the model. We are going to store the results in a csv file called `all_results.csv` under the `results/time-moe` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n",
    "Note that we try to replace the results with the baseline results whenever the model yield nan forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fdfbb-31bb-44d2-b511-a9ea8fba5402",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T20:57:34.953643Z",
     "iopub.status.busy": "2025-02-18T20:57:34.953042Z",
     "iopub.status.idle": "2025-02-18T20:58:17.872060Z",
     "shell.execute_reply": "2025-02-18T20:58:17.871476Z",
     "shell.execute_reply.started": "2025-02-18T20:57:34.953621Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from gluonts.model import evaluate_model\n",
    "import csv\n",
    "import os\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from gift_eval.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Iterate over all available datasets\n",
    "model_name = \"time-moe\"\n",
    "output_dir = f\"ChangeMe/{model_name}\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"eval_metrics/MAAPE[0.5]\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "            \"dataset_size\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "    dataset_term = dataset_properties_map[ds_name][\"term\"]\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or term == \"long\") and dataset_term == \"short\":\n",
    "            continue\n",
    "        ds_freq = dataset_properties_map[ds_name][\"frequency\"]\n",
    "        ds_config = f\"{ds_name}/{ds_freq}/{term}\"\n",
    "\n",
    "        # Initialize the dataset, since Moirai support multivariate time series forecast, it does not require\n",
    "        # to convert the original data into univariate\n",
    "        to_univariate = False if Dataset(name=ds_name, term=term,to_univariate=False,storage_env_var=\"BOOM\").target_dim == 1 else True\n",
    "        # to_univariate = False\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate,storage_env_var=\"BOOM\")\n",
    "        model = timemoe_wrapper(dataset.prediction_length)\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        dataset_size = len(dataset.test_data)\n",
    "        print(f\"Dataset size: {dataset_size}\")\n",
    "        \n",
    "        try:\n",
    "            res = evaluate_model(\n",
    "                model,\n",
    "                test_data=dataset.test_data,\n",
    "                metrics=metrics,\n",
    "                batch_size=512,\n",
    "                axis=None,\n",
    "                mask_invalid_label=True,\n",
    "                allow_nan_forecast=False,\n",
    "                seasonality=season_length,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"NaN\" in str(e):\n",
    "                print(f\"replacing results of {ds_name} with seasonal naive scores due to NaN values\")\n",
    "                res = pd.read_csv(f\"ChangeMe/seasonalnaive/all_results.csv\")\n",
    "                prefix = \"eval_metrics/\"\n",
    "                res.columns = [col[len(prefix):] if col.startswith(prefix) else col for col in res.columns]\n",
    "                res = res[res[\"dataset\"]==ds_config]\n",
    "                res = res.reset_index(drop=True)\n",
    "            else:\n",
    "                raise e\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    \"time-moe\",\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    res[\"MAAPE[0.5]\"][0],\n",
    "                    dataset_properties_map[ds_name][\"domain\"],\n",
    "                    dataset_properties_map[ds_name][\"num_variates\"],\n",
    "                    dataset_size,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1564c38",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/time-moe` folder containing the results for the Time-MOE model on the boom benchmark. The csv file will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afdd29-8ee8-4edd-8991-f3da0a5063b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-02-18T20:58:58.571355Z",
     "iopub.status.busy": "2025-02-18T20:58:58.570937Z",
     "iopub.status.idle": "2025-02-18T20:58:58.627884Z",
     "shell.execute_reply": "2025-02-18T20:58:58.627351Z",
     "shell.execute_reply.started": "2025-02-18T20:58:58.571334Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_dir + \"/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "dd-sharing": {
   "allowed_groups": [
    "subproduct-datascience",
    "combined-data-science",
    "team-largemodelfoundationsresearch",
    ""
   ],
   "allowed_users": [
    ""
   ],
   "retention_period": "90"
  },
  "kernelspec": {
   "display_name": "time-moe_eval_env",
   "language": "python",
   "name": "time-moe_eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
