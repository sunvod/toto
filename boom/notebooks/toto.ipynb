{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "# Quick Start: Running Foundation Model Toto on BOOM benchmark\n",
    "\n",
    "This notebook shows how to run the Toto on the BOOM benchmark.\n",
    "\n",
    "Make sure you download the BOOM benchmark and set the `BOOM` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class from GiftEval to load the data and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-12-02T22:51:39.780107Z",
     "iopub.status.busy": "2024-12-02T22:51:39.779779Z",
     "iopub.status.idle": "2024-12-02T22:51:39.798046Z",
     "shell.execute_reply": "2024-12-02T22:51:39.797589Z",
     "shell.execute_reply.started": "2024-12-02T22:51:39.780086Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the working directory to the Toto module\n",
    "%cd ../../toto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download BOOM datasets. Calling `download_boom_benchmark` also sets the `BOOM` environment variable with the correct path, which is needed for running the evals below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-12-02T22:51:39.798869Z",
     "iopub.status.busy": "2024-12-02T22:51:39.798645Z",
     "iopub.status.idle": "2024-12-02T22:51:44.183029Z",
     "shell.execute_reply": "2024-12-02T22:51:44.182337Z",
     "shell.execute_reply.started": "2024-12-02T22:51:39.798849Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from gift_eval.data import Dataset\n",
    "from dataset_utils import download_boom_benchmark\n",
    "from model.toto import Toto\n",
    "from inference.gluonts_predictor import TOTOPredictor, Multivariate\n",
    "import torch\n",
    "\n",
    "boom_path = \"ChangeMe\"\n",
    "download_boom_benchmark(boom_path)\n",
    "load_dotenv()\n",
    "\n",
    "dataset_properties_map = json.load(open(\"../boom/dataset_properties.json\"))\n",
    "all_datasets = list(dataset_properties_map.keys())\n",
    "print(len(all_datasets))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-12-02T22:51:44.184291Z",
     "iopub.status.busy": "2024-12-02T22:51:44.183808Z",
     "iopub.status.idle": "2024-12-02T22:51:44.224645Z",
     "shell.execute_reply": "2024-12-02T22:51:44.224121Z",
     "shell.execute_reply.started": "2024-12-02T22:51:44.184269Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MSE,\n",
    "    MAE,\n",
    "    MASE,\n",
    "    MAPE,\n",
    "    SMAPE,\n",
    "    MSIS,\n",
    "    RMSE,\n",
    "    NRMSE,\n",
    "    ND,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Toto Predictor\n",
    "Load Toto model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-12-02T22:51:44.225572Z",
     "iopub.status.busy": "2024-12-02T22:51:44.225273Z",
     "iopub.status.idle": "2024-12-02T22:51:46.694172Z",
     "shell.execute_reply": "2024-12-02T22:51:46.693556Z",
     "shell.execute_reply.started": "2024-12-02T22:51:44.225551Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "toto = Toto.from_pretrained('Datadog/Toto-Open-Base-1.0')\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "toto.to(DEVICE)\n",
    "\n",
    "\n",
    "class TOTOModelPredictorWrapper:\n",
    "    def __init__(self, model, prediction_length, context_length, mode, num_samples=128, use_kv_cache=True):\n",
    "        \"\"\"\n",
    "        Initialize the predictor wrapper with specified parameters.\n",
    "\n",
    "        Args:\n",
    "            model: The PyTorch model to be used for predictions.\n",
    "            prediction_length: The length of the prediction horizon.\n",
    "            context_length: The length of the context window.\n",
    "            mode: Mode of prediction (e.g., \"forecast\").\n",
    "            initial_samples_per_batch: Starting batch size for predictions (even number).\n",
    "            num_samples: Total number of samples to generate.\n",
    "            use_kv_cache: Whether to use key-value caching.\n",
    "        \"\"\"\n",
    "        self.model = torch.compile(model)\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.mode = mode\n",
    "        self.num_samples = num_samples\n",
    "        self.use_kv_cache = use_kv_cache\n",
    "        self.samples_per_batch = num_samples\n",
    "        self.model = model\n",
    "        self.predictor = None\n",
    "        self._adjusted = False  # Tracks whether adjustment has been done\n",
    "\n",
    "        self._initialize_predictor()\n",
    "\n",
    "    def _initialize_predictor(self):\n",
    "        \"\"\"\n",
    "        Initialize the TOTOPredictor with the current samples_per_batch.\n",
    "        \"\"\"\n",
    "        self.predictor = TOTOPredictor.create_for_eval(\n",
    "            model=self.model,\n",
    "            prediction_length=self.prediction_length,\n",
    "            context_length=self.context_length,\n",
    "            mode=self.mode,\n",
    "            samples_per_batch=self.samples_per_batch,\n",
    "        )\n",
    "\n",
    "    def predict(self, gluonts_test_data: tuple):\n",
    "        \"\"\"\n",
    "        Perform prediction while adjusting num_samples and samples_per_batch if OOM errors occur.\n",
    "        \"\"\"\n",
    "        # Adjust samples_per_batch only on the first prediction call\n",
    "        if not self._adjusted:\n",
    "            print(\"Initializing predictor with samples_per_batch =\", self.samples_per_batch)\n",
    "\n",
    "            while self.samples_per_batch >= 1:\n",
    "                try:\n",
    "                    print(f\"Attempting prediction with samples_per_batch = {self.samples_per_batch}\")\n",
    "                    # Consume the generator here to catch any exceptions\n",
    "                    predictions = list(\n",
    "                        self.predictor.predict(\n",
    "                            gluonts_test_data, use_kv_cache=self.use_kv_cache, num_samples=self.num_samples\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    self._adjusted = True\n",
    "\n",
    "                    return predictions  # Success\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"exception {self._adjusted=}\")\n",
    "                    print(\"Caught exception during prediction.\")\n",
    "                    if \"CUDA out of memory\" in str(e):\n",
    "                        print(\n",
    "                            f\"Out of memory with samples_per_batch = {self.samples_per_batch}. Reducing samples_per_batch.\"\n",
    "                        )\n",
    "                        torch.cuda.empty_cache()  # Clear cache before retrying\n",
    "                        self.samples_per_batch = self.samples_per_batch // 2\n",
    "                        if self.samples_per_batch < 1:\n",
    "                            raise RuntimeError(\n",
    "                                \"Unable to perform prediction even with minimal samples_per_batch due to OOM.\"\n",
    "                            )\n",
    "                        self._initialize_predictor()\n",
    "                    else:\n",
    "                        raise e  # Re-raise unexpected exceptions\n",
    "        # For subsequent calls, we can just return the generator\n",
    "        return self.predictor.predict(gluonts_test_data, use_kv_cache=self.use_kv_cache, num_samples=self.num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the boom benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/toto` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T22:51:46.877403Z",
     "iopub.status.idle": "2024-12-02T22:51:46.877645Z",
     "shell.execute_reply": "2024-12-02T22:51:46.877535Z",
     "shell.execute_reply.started": "2024-12-02T22:51:46.877524Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "default_context_length = 2048\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Iterate over all available datasets\n",
    "\n",
    "output_dir = \"../results/toto\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "toto_model = Toto.from_pretrained('Datadog/Toto-Open-Base-1.0').to(DEVICE)\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name}\")\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or term == \"long\") and ds_name not in med_long_datasets.split():\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "\n",
    "        # Initialize the dataset, since Toto support multivariate time series forecast, it does not require\n",
    "        # to convert the original data into univariate\n",
    "        # to_univariate = False if Dataset(name=ds_name, term=term,to_univariate=False).target_dim == 1 else True\n",
    "        to_univariate = False\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "        for i, j in enumerate(dataset.test_data):\n",
    "            break\n",
    "\n",
    "        if len(j[0][\"target\"].shape) == 2:\n",
    "            batch_size, context_length = j[0][\"target\"].shape\n",
    "        else:\n",
    "            batch_size = 1\n",
    "            context_length = j[0][\"target\"].shape[0]\n",
    "\n",
    "        predictor_wrapper = TOTOModelPredictorWrapper(\n",
    "            model=toto_model,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "            context_length=min(default_context_length, context_length),\n",
    "            mode=Multivariate(batch_size=batch_size),  # Adjust based on use case\n",
    "        )\n",
    "\n",
    "        # Pad the test_data\n",
    "        print(f\"{ds_name =}, {term=}, {ds_key=}\")\n",
    "\n",
    "        # Determine seasonality\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "\n",
    "        # Evaluate results\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        res = evaluate_model(\n",
    "            predictor_wrapper,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=512,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    \"toto\",\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_key][\"domain\"],\n",
    "                    dataset_properties_map[ds_key][\"num_variates\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/Toto` folder containing the results for the Toto model on the boom benchmark. The csv file will look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.status.busy": "2024-12-02T22:51:46.878529Z",
     "iopub.status.idle": "2024-12-02T22:51:46.878769Z",
     "shell.execute_reply": "2024-12-02T22:51:46.878654Z",
     "shell.execute_reply.started": "2024-12-02T22:51:46.878644Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/toto/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "dd-sharing": {
   "allowed_groups": [
    "combined-data-science",
    "team-datascience-frontlinellmfoundations",
    "subproduct-aiagentfoundations",
    ""
   ],
   "allowed_users": [
    ""
   ],
   "retention_period": "90"
  },
  "kernelspec": {
   "display_name": "toto_eval_env",
   "language": "python",
   "name": "toto_eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
