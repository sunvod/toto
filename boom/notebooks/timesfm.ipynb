{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "# Quick Start: Running timesfm models on gift-eval benchmark\n",
    "\n",
    "This notebook is adapted from the [GiftEval repository](https://github.com/SalesforceAIResearch/gift-eval/tree/main/notebooks) and shows how to run Timesfm-2.0 on BOOM.\n",
    "\n",
    "Make sure you download the BOOM benchmark and set the `BOOM` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download BOOM datasets. Calling `download_boom_benchmark` also sets the `BOOM` environment variable with the correct path, which is needed for running the evals below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T18:09:28.566511Z",
     "iopub.status.busy": "2025-04-20T18:09:28.565859Z",
     "iopub.status.idle": "2025-04-20T18:09:31.295580Z",
     "shell.execute_reply": "2025-04-20T18:09:31.295008Z",
     "shell.execute_reply.started": "2025-04-20T18:09:28.566470Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from dataset_utils import download_boom_benchmark\n",
    "\n",
    "boom_path = \"ChangeMe\"\n",
    "download_boom_benchmark(boom_path)\n",
    "load_dotenv()\n",
    "\n",
    "dataset_properties_map = json.load(open(\"boom/dataset_properties.json\"))\n",
    "all_datasets = list(dataset_properties_map.keys())\n",
    "print(len(all_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T18:09:32.853220Z",
     "iopub.status.busy": "2025-04-20T18:09:32.852732Z",
     "iopub.status.idle": "2025-04-20T18:09:32.892804Z",
     "shell.execute_reply": "2025-04-20T18:09:32.892278Z",
     "shell.execute_reply.started": "2025-04-20T18:09:32.853195Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "## TimesFM Predictor\n",
    "\n",
    "For foundation models, we need to implement a wrapper containing the model and use the wrapper to generate predicitons.\n",
    "\n",
    "This is just meant to be a simple wrapper to get you started, feel free to use your own custom implementation to wrap any model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "### Lets first load the timesfm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T18:09:35.495812Z",
     "iopub.status.busy": "2025-04-20T18:09:35.495461Z",
     "iopub.status.idle": "2025-04-20T18:10:02.615880Z",
     "shell.execute_reply": "2025-04-20T18:10:02.614777Z",
     "shell.execute_reply.started": "2025-04-20T18:09:35.495787Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import timesfm\n",
    "\n",
    "# tfm = timesfm.TimesFm(\n",
    "#     hparams=timesfm.TimesFmHparams(\n",
    "#         backend=\"gpu\",\n",
    "#         per_core_batch_size=32,\n",
    "#         num_layers=50,\n",
    "#         horizon_len=128,\n",
    "#         context_len=2048,\n",
    "#         use_positional_embedding=False,\n",
    "#         output_patch_len=128,\n",
    "#     ),\n",
    "#     checkpoint=timesfm.TimesFmCheckpoint(huggingface_repo_id=\"google/timesfm-2.0-500m-jax\"),\n",
    "# )\n",
    "\n",
    "# If you are using the pytorch version:\n",
    "tfm = timesfm.TimesFm(\n",
    "    hparams=timesfm.TimesFmHparams(\n",
    "        backend=\"gpu\",\n",
    "        per_core_batch_size=32,\n",
    "        num_layers=50,\n",
    "        horizon_len=128,\n",
    "        context_len=2048,\n",
    "        use_positional_embedding=False,\n",
    "        output_patch_len=128,\n",
    "    ),\n",
    "    checkpoint=timesfm.TimesFmCheckpoint(\n",
    "        huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T18:10:02.618085Z",
     "iopub.status.busy": "2025-04-20T18:10:02.617306Z",
     "iopub.status.idle": "2025-04-20T18:10:02.672778Z",
     "shell.execute_reply": "2025-04-20T18:10:02.672257Z",
     "shell.execute_reply.started": "2025-04-20T18:10:02.618057Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "\n",
    "\n",
    "class TimesFmPredictor:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tfm,\n",
    "        prediction_length: int,\n",
    "        ds_freq: str,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.tfm = tfm\n",
    "        self.prediction_length = prediction_length\n",
    "        if self.prediction_length > self.tfm.horizon_len:\n",
    "            self.tfm.horizon_len = (\n",
    "                (self.prediction_length + self.tfm.output_patch_len - 1) // self.tfm.output_patch_len\n",
    "            ) * self.tfm.output_patch_len\n",
    "            print(\"Jitting for new prediction length.\")\n",
    "        self.freq = timesfm.freq_map(ds_freq)\n",
    "        print(\"frequency key:\", ds_freq)\n",
    "        print(\"frequency:\", self.freq)\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 128) -> List[Forecast]:\n",
    "        forecast_outputs = []\n",
    "        for batch in tqdm(batcher(test_data_input, batch_size=batch_size)):\n",
    "            context = []\n",
    "            for entry in batch:\n",
    "                arr = np.array(entry[\"target\"])\n",
    "                context.append(arr)\n",
    "            freqs = [self.freq] * len(context)\n",
    "            _, full_preds = self.tfm.forecast(context, freqs, normalize=True)\n",
    "            full_preds = full_preds[:, 0 : self.prediction_length, 1:]\n",
    "            forecast_outputs.append(full_preds.transpose((0, 2, 1)))\n",
    "        forecast_outputs = np.concatenate(forecast_outputs)\n",
    "\n",
    "        # Convert forecast samples into gluonts Forecast objects\n",
    "        forecasts = []\n",
    "        for item, ts in zip(forecast_outputs, test_data_input):\n",
    "            forecast_start_date = ts[\"start\"] + len(ts[\"target\"])\n",
    "            forecasts.append(\n",
    "                QuantileForecast(\n",
    "                    forecast_arrays=item,\n",
    "                    forecast_keys=list(map(str, self.tfm.quantiles)),\n",
    "                    start_date=forecast_start_date,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/timesfm_2_0_500m` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n",
    "Note that we try to replace the results with the baseline results whenever the model yield nan forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T18:10:02.673568Z",
     "iopub.status.busy": "2025-04-20T18:10:02.673362Z",
     "iopub.status.idle": "2025-04-20T18:10:02.705202Z",
     "shell.execute_reply": "2025-04-20T18:10:02.704721Z",
     "shell.execute_reply.started": "2025-04-20T18:10:02.673549Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(WarningFilter(\"The mean prediction is not stored in the forecast data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "### Ordering all dataset settings from lowest to highest prediction length to minimize the number of jittings. This is not necessary for the pytorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T18:10:02.706352Z",
     "iopub.status.busy": "2025-04-20T18:10:02.706112Z",
     "iopub.status.idle": "2025-04-20T18:10:02.739077Z",
     "shell.execute_reply": "2025-04-20T18:10:02.738600Z",
     "shell.execute_reply.started": "2025-04-20T18:10:02.706333Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from gift_eval.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "### Evaluating on all settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T18:10:32.491891Z",
     "iopub.status.busy": "2025-04-20T18:10:32.491530Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "model_name = \"timesfm_2_0_500m\"\n",
    "output_dir = f\"ChangeMe/{model_name}\"\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write the header\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"dataset\",\n",
    "            \"model\",\n",
    "            \"eval_metrics/MSE[mean]\",\n",
    "            \"eval_metrics/MSE[0.5]\",\n",
    "            \"eval_metrics/MAE[0.5]\",\n",
    "            \"eval_metrics/MASE[0.5]\",\n",
    "            \"eval_metrics/MAPE[0.5]\",\n",
    "            \"eval_metrics/sMAPE[0.5]\",\n",
    "            \"eval_metrics/MSIS\",\n",
    "            \"eval_metrics/RMSE[mean]\",\n",
    "            \"eval_metrics/NRMSE[mean]\",\n",
    "            \"eval_metrics/ND[0.5]\",\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "            \"domain\",\n",
    "            \"num_variates\",\n",
    "            \"dataset_size\",\n",
    "        ]\n",
    "    )\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    dataset_term = dataset_properties_map[ds_name][\"term\"]\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or term == \"long\") and dataset_term == \"short\":\n",
    "            continue\n",
    "        ds_freq = dataset_properties_map[ds_name][\"frequency\"]\n",
    "        ds_config = f\"{ds_name}/{ds_freq}/{term}\"\n",
    "        to_univariate = False if Dataset(name=ds_name, term=term, to_univariate=False,storage_env_var=\"BOOM\").target_dim == 1 else True\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate,storage_env_var=\"BOOM\")\n",
    "        season_length = get_seasonality(dataset.freq)\n",
    "        dataset_size = len(dataset.test_data)\n",
    "        print(f\"Dataset size: {dataset_size}\")\n",
    "        predictor = TimesFmPredictor(\n",
    "            tfm=tfm,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "            ds_freq=dataset.freq,\n",
    "        )\n",
    "        # Measure the time taken for evaluation\n",
    "        try:\n",
    "            res = evaluate_model(\n",
    "                predictor,\n",
    "                test_data=dataset.test_data,\n",
    "                metrics=metrics,\n",
    "                batch_size=1024,\n",
    "                axis=None,\n",
    "                mask_invalid_label=True,\n",
    "                allow_nan_forecast=False,\n",
    "                seasonality=season_length,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"NaN\" in str(e):\n",
    "                print(f\"replacing results of {ds_name} with seasonal naive scores due to NaN values\")\n",
    "                res = pd.read_csv(f\"ChangeMe/seasonalnaive/all_results.csv\")\n",
    "                prefix = \"eval_metrics/\"\n",
    "                res.columns = [col[len(prefix):] if col.startswith(prefix) else col for col in res.columns]\n",
    "                res = res[res[\"dataset\"]==ds_config]\n",
    "                res = res.reset_index(drop=True)\n",
    "            else:\n",
    "                raise e \n",
    "    \n",
    "        # Append the results to the CSV file\n",
    "        with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    ds_config,\n",
    "                    model_name,\n",
    "                    res[\"MSE[mean]\"][0],\n",
    "                    res[\"MSE[0.5]\"][0],\n",
    "                    res[\"MAE[0.5]\"][0],\n",
    "                    res[\"MASE[0.5]\"][0],\n",
    "                    res[\"MAPE[0.5]\"][0],\n",
    "                    res[\"sMAPE[0.5]\"][0],\n",
    "                    res[\"MSIS\"][0],\n",
    "                    res[\"RMSE[mean]\"][0],\n",
    "                    res[\"NRMSE[mean]\"][0],\n",
    "                    res[\"ND[0.5]\"][0],\n",
    "                    res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "                    dataset_properties_map[ds_name][\"domain\"],\n",
    "                    dataset_properties_map[ds_name][\"num_variates\"],\n",
    "                    dataset_size,\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "        print(f\"Results for {ds_name} have been written to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/timesfm` folder containing the results for the Chronos model on the gift-eval benchmark. We can display the csv file using the follow code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-16T17:48:25.064478Z",
     "iopub.status.busy": "2025-04-16T17:48:25.064131Z",
     "iopub.status.idle": "2025-04-16T17:48:25.124111Z",
     "shell.execute_reply": "2025-04-16T17:48:25.123437Z",
     "shell.execute_reply.started": "2025-04-16T17:48:25.064455Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_dir + \"/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "dd-sharing": {
   "allowed_groups": [
    "subproduct-datascience",
    "combined-data-science",
    "team-largemodelfoundationsresearch",
    ""
   ],
   "allowed_users": [
    ""
   ],
   "retention_period": "90"
  },
  "kernelspec": {
   "display_name": "timesfm_eval_env",
   "language": "python",
   "name": "timesfm_eval_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
