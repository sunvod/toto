{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "<div style=\"background-color: #f2ecfc; padding: 10px; border-radius: 5px; font-family: Helvetica, Arial, sans-serif;\">\n",
    "\n",
    "# Quick Start: Running Foundation Model üê∂ <b>Toto</b> on GIFT-eval benchmark\n",
    "\n",
    "This notebook shows how to run the Toto on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the GIFT-Eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "Before proceeding, ensure you have the following:\n",
    "\n",
    "1. **Clone the Toto Repository**\n",
    "\n",
    "   Open your terminal and execute:\n",
    "\n",
    "   ```bash\n",
    "   git clone https://github.com/DataDog/toto.git\n",
    "   cd toto\n",
    "   ```\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:20:57.277581Z",
     "iopub.status.busy": "2025-05-15T19:20:57.277225Z",
     "iopub.status.idle": "2025-05-15T19:21:03.504890Z",
     "shell.execute_reply": "2025-05-15T19:21:03.504262Z",
     "shell.execute_reply.started": "2025-05-15T19:20:57.277556Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/venvs/toto-test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))\n",
    "os.environ[\"GIFT_EVAL\"] = \"Change/To/GiftEval/Local/Path\"\n",
    "\n",
    "# Standard library imports\n",
    "import gc\n",
    "\n",
    "# Third-party imports\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Local imports\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "from gift_eval.data import Dataset as GiftEvalDataset\n",
    "\n",
    "from inference.gluonts_predictor import Multivariate, TotoPredictor\n",
    "from model.toto import Toto\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:03.506259Z",
     "iopub.status.busy": "2025-05-15T19:21:03.505819Z",
     "iopub.status.idle": "2025-05-15T19:21:03.549366Z",
     "shell.execute_reply": "2025-05-15T19:21:03.548870Z",
     "shell.execute_reply.started": "2025-05-15T19:21:03.506236Z"
    },
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET_PROPERTIES_PATH = \"./dataset_properties.json\"\n",
    "\n",
    "DEFAULT_CONTEXT_LENGTH = 4096\n",
    "\n",
    "PRETTY_DATASET_NAMES = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "# SHORT_DATASETS = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "# MED_LONG_DATASETS = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "SHORT_DATASETS = \"m4_weekly\"\n",
    "MED_LONG_DATASETS = \"bizitobs_l2c/H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:03.550190Z",
     "iopub.status.busy": "2025-05-15T19:21:03.549940Z",
     "iopub.status.idle": "2025-05-15T19:21:03.594769Z",
     "shell.execute_reply": "2025-05-15T19:21:03.594243Z",
     "shell.execute_reply.started": "2025-05-15T19:21:03.550172Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Define metrics configuration once at module level\n",
    "METRIC_CONFIGS = {\n",
    "    \"MAE\": (lambda: MAE(), \"MAE[0.5]\"),\n",
    "    \"MSE\": (lambda: MSE(forecast_type=0.5), \"MSE[0.5]\"),\n",
    "    \"MSE_MEAN\": (lambda: MSE(forecast_type=\"mean\"), \"MSE[mean]\"),\n",
    "    \"MASE\": (lambda: MASE(), \"MASE[0.5]\"),\n",
    "    \"MAPE\": (lambda: MAPE(), \"MAPE[0.5]\"),\n",
    "    \"SMAPE\": (lambda: SMAPE(), \"sMAPE[0.5]\"),\n",
    "    \"MSIS\": (lambda: MSIS(), \"MSIS\"),\n",
    "    \"RMSE\": (lambda: RMSE(forecast_type=0.5), \"RMSE[0.5]\"),\n",
    "    \"RMSE_MEAN\": (lambda: RMSE(forecast_type=\"mean\"), \"RMSE[mean]\"),\n",
    "    \"NRMSE\": (lambda: NRMSE(forecast_type=0.5), \"NRMSE[0.5]\"),\n",
    "    \"NRMSE_MEAN\": (lambda: NRMSE(forecast_type=\"mean\"), \"NRMSE[mean]\"),\n",
    "    \"ND\": (lambda: ND(), \"ND[0.5]\"),\n",
    "    \"WQTL\": (\n",
    "        lambda: MeanWeightedSumQuantileLoss(\n",
    "            quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        ),\n",
    "        \"mean_weighted_sum_quantile_loss\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "<div style=\"background-color: #f2ecfc; padding: 10px; border-radius: 5px; font-family: Helvetica, Arial, sans-serif;\">\n",
    "\n",
    "## üê∂ Toto Predictor\n",
    "Load <b>Toto</b> model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:04.644703Z",
     "iopub.status.busy": "2025-05-15T19:21:04.644339Z",
     "iopub.status.idle": "2025-05-15T19:21:04.720992Z",
     "shell.execute_reply": "2025-05-15T19:21:04.720430Z",
     "shell.execute_reply.started": "2025-05-15T19:21:04.644679Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class EvalTask:\n",
    "    \"\"\"Dataclass representing an evaluation task with all necessary parameters.\"\"\"\n",
    "\n",
    "    dataset_name: str\n",
    "    term: str\n",
    "    checkpoint_path: str\n",
    "    num_samples: int\n",
    "    use_kv_cache: bool\n",
    "    seed: int\n",
    "    dataset_properties_map: Dict[str, Any]\n",
    "    dataset_frequency: str\n",
    "    dataset_key: str\n",
    "    evaluation_target: str = \"test\"  # Can be \"test\" or \"validation\"\n",
    "    pad_short_series: bool = False\n",
    "\n",
    "\n",
    "def get_total_gpu_memory():\n",
    "    \"\"\"Get total GPU VRAM capacity in MB.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.cuda.current_device()\n",
    "    return torch.cuda.get_device_properties(device).total_memory / (1024 * 1024)\n",
    "\n",
    "\n",
    "def calculate_optimal_batch_size(\n",
    "    model,\n",
    "    target_dim,\n",
    "    prediction_length,\n",
    "    context_length,\n",
    "    use_kv_cache,\n",
    "    num_samples,\n",
    "    safety_factor=0.01,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the optimal batch size based on available GPU memory and model requirements.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-loaded TOTO model\n",
    "        target_dim: Target dimensionality (number of variates)\n",
    "        prediction_length: Length of prediction horizon\n",
    "        context_length: Context window length\n",
    "        use_kv_cache: Whether KV cache is used\n",
    "        num_samples: Number of samples to generate\n",
    "        safety_factor: Safety factor to apply when calculating available memory (default=0.01)\n",
    "\n",
    "    Returns:\n",
    "        Suggested batch size for prediction\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Extract model size information\n",
    "        model_width = model.model.embed_dim  # Feature dimension\n",
    "        model_depth = model.model.num_layers  # Number of transformer layers\n",
    "\n",
    "        # Calculate model's parameter memory footprint in MB\n",
    "        model_param_memory_mb = sum(\n",
    "            p.numel() * p.element_size() for p in model.parameters()\n",
    "        ) / (1024 * 1024)\n",
    "\n",
    "        # Base memory per sample in MB (parameters + activations + gradients)\n",
    "        base_memory_per_sample = (model_width * model_depth * 4) / (1024 * 1024)\n",
    "\n",
    "        # Memory for input/output tensors\n",
    "        io_memory = (target_dim * (context_length + prediction_length) * 4) / (\n",
    "            1024 * 1024\n",
    "        )\n",
    "\n",
    "        # KV cache memory (if used)\n",
    "        kv_memory = 0\n",
    "        if use_kv_cache:\n",
    "            kv_memory = (model_depth * model_width * 2 * context_length * 4) / (\n",
    "                1024 * 1024\n",
    "            )\n",
    "\n",
    "        # Total memory per sample\n",
    "        mem_per_sample_mb = base_memory_per_sample + io_memory + kv_memory\n",
    "\n",
    "        # Factor in target dimensions and samples directly\n",
    "        # Each dimension and sample has a direct multiplicative effect on memory\n",
    "        mem_per_batch_mb = (\n",
    "            mem_per_sample_mb * target_dim * num_samples\n",
    "        )  # Total memory for a batch with num_samples samples\n",
    "\n",
    "        # Get total GPU VRAM capacity and subtract model parameter memory\n",
    "        gpu_mem = get_total_gpu_memory()  # in MB\n",
    "        cuda_reserved_mb = 1024  # Reserve 1GB for CUDA runtime and other overhead\n",
    "\n",
    "        # Available memory = (Total VRAM - Model parameters - CUDA reserved) * safety factor\n",
    "        available_memory = (\n",
    "            gpu_mem - model_param_memory_mb - cuda_reserved_mb\n",
    "        ) * safety_factor\n",
    "\n",
    "        # Calculate max batch size based on available memory\n",
    "        max_batch_size = max(\n",
    "            1, int(available_memory / (mem_per_batch_mb / num_samples))\n",
    "        )\n",
    "\n",
    "        max_batch_size = min(16, max_batch_size)\n",
    "        return max_batch_size\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error calculating optimal batch size: {e}\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_maximal_context_length(dataset: GiftEvalDataset):\n",
    "    \"\"\"\n",
    "    Calculates the maximal context length that can be used for the given dataset,\n",
    "    based on the shortest time series in the dataset and the number of\n",
    "    prediction windows used for validation and testing.\n",
    "\n",
    "    The context length is computed by subtracting the total number of\n",
    "    prediction steps (across test and val windows) from the shortest\n",
    "    time series length in the dataset.\n",
    "    \"\"\"\n",
    "    shortest_series_in_dataset = dataset._min_series_length\n",
    "    total_prediction_windows = (\n",
    "        dataset.windows + 1\n",
    "    )  # dataset.windows is the number of windows in the rolling evaluation in the test split, and 1 is the prediction window we leave out in the validation split -> everything else before can be used as context data\n",
    "    max_context_length = (\n",
    "        shortest_series_in_dataset\n",
    "        - total_prediction_windows * dataset.prediction_length\n",
    "    )  # total series length - (number of eval windows + 1) * eval window length\n",
    "    return max_context_length\n",
    "\n",
    "\n",
    "def prepare_evaluation_data(\n",
    "    dataset: GiftEvalDataset, base_dataset, prediction_length: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to prepare evaluation data by splitting a dataset and generating instances.\n",
    "\n",
    "    Args:\n",
    "        dataset: The GiftEvalDataset instance containing dataset metadata\n",
    "        base_dataset: The base dataset to split (training or validation dataset)\n",
    "        prediction_length: The prediction horizon length\n",
    "\n",
    "    Returns:\n",
    "        Generated evaluation data ready for model evaluation\n",
    "    \"\"\"\n",
    "    # Determine the number of validation windows based on dataset type\n",
    "    if \"m4\" in dataset.name:\n",
    "        # Special case for M4 datasets\n",
    "        validation_windows = 1\n",
    "        print(f\"M4 dataset detected: using {validation_windows} window\")\n",
    "    else:\n",
    "        # Use the same windows count as in the dataset\n",
    "        validation_windows = dataset.windows\n",
    "        print(f\"Using dataset.windows = {validation_windows} windows for evaluation\")\n",
    "\n",
    "    # Split the dataset and create evaluation instances\n",
    "    _, test_template = split(base_dataset, offset=-prediction_length * dataset.windows)\n",
    "\n",
    "    # Generate instances for evaluation\n",
    "    evaluation_data = test_template.generate_instances(\n",
    "        prediction_length=prediction_length,\n",
    "        windows=validation_windows,\n",
    "        distance=prediction_length,\n",
    "    )\n",
    "\n",
    "    return evaluation_data\n",
    "\n",
    "\n",
    "class TOTOModelPredictorWrapper:\n",
    "    \"\"\"Wrapper for TOTOPredictor that handles OOM errors by adjusting batch size.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        prediction_length,\n",
    "        context_length,\n",
    "        mode,\n",
    "        num_samples=128,\n",
    "        use_kv_cache=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the predictor wrapper with specified parameters.\n",
    "\n",
    "        Args:\n",
    "            model: The loaded TOTO model instance to use for predictions\n",
    "            prediction_length: The length of the prediction horizon.\n",
    "            context_length: The length of the context window.\n",
    "            mode: Mode of prediction (e.g., \"forecast\").\n",
    "            num_samples: Total number of samples to generate.\n",
    "            use_kv_cache: Whether to use key-value caching.\n",
    "        \"\"\"\n",
    "\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.mode = mode\n",
    "        self.num_samples = num_samples\n",
    "        self.use_kv_cache = use_kv_cache\n",
    "        self.samples_per_batch = (\n",
    "            num_samples  # Start with full batch size and adjust if needed\n",
    "        )\n",
    "        self.model = model\n",
    "        self._adjusted = False  # Tracks whether adjustment has been done\n",
    "\n",
    "        self._initialize_predictor()\n",
    "\n",
    "    def _initialize_predictor(self):\n",
    "        \"\"\"\n",
    "        Initialize the TOTOPredictor with the current samples_per_batch.\n",
    "        \"\"\"\n",
    "        self.predictor = TotoPredictor.create_for_eval(\n",
    "            model=self.model,\n",
    "            prediction_length=self.prediction_length,\n",
    "            context_length=self.context_length,\n",
    "            mode=self.mode,\n",
    "            samples_per_batch=self.samples_per_batch,\n",
    "        )\n",
    "\n",
    "    def predict(self, gluonts_test_data: tuple):\n",
    "        \"\"\"\n",
    "        Perform prediction while adjusting num_samples, samples_per_batch, and context_length if OOM errors occur.\n",
    "        \"\"\"\n",
    "        predictions = None\n",
    "\n",
    "        # Adjust only on the first call.\n",
    "        if not self._adjusted:\n",
    "\n",
    "            print(\n",
    "                \"Initializing predictor with samples_per_batch =\",\n",
    "                self.samples_per_batch,\n",
    "            )\n",
    "            while self.samples_per_batch >= 1:\n",
    "                try:\n",
    "                    print(\n",
    "                        f\"Attempting prediction with samples_per_batch = {self.samples_per_batch} and context_length = {self.context_length}\"\n",
    "                    )\n",
    "                    # Attempt prediction (consume the generator to catch any OOM)\n",
    "                    predictions = list(\n",
    "                        self.predictor.predict(\n",
    "                            gluonts_test_data,\n",
    "                            use_kv_cache=self.use_kv_cache,\n",
    "                            num_samples=self.num_samples,\n",
    "                        )\n",
    "                    )\n",
    "                    self._adjusted = True\n",
    "                    return predictions  # Prediction succeeded\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    print(\"Caught exception during prediction:\", e)\n",
    "                    if \"CUDA out of memory\" in str(e):\n",
    "                        # First, try reducing the batch size if possible.\n",
    "                        if self.samples_per_batch > 1:\n",
    "                            print(\n",
    "                                f\"Out of memory with samples_per_batch = {self.samples_per_batch}. Reducing batch size.\"\n",
    "                            )\n",
    "                            self.samples_per_batch = self.samples_per_batch // 2\n",
    "                            # Clean up GPU memory before trying with smaller batch size\n",
    "                            torch.cuda.empty_cache()\n",
    "                        else:\n",
    "                            # Cannot reduce batch size further, so we fail\n",
    "                            print(\n",
    "                                f\"OOM at minimal batch size. Cannot proceed with this context length and sample count.\"\n",
    "                            )\n",
    "                            raise e\n",
    "                        # Reinitialize the predictor with the new settings.\n",
    "                        self._initialize_predictor()\n",
    "                    else:\n",
    "                        raise e  # Re-raise unexpected exceptions\n",
    "\n",
    "        # For subsequent calls, simply return the generator.\n",
    "        return self.predictor.predict(\n",
    "            gluonts_test_data,\n",
    "            use_kv_cache=self.use_kv_cache,\n",
    "            num_samples=self.num_samples,\n",
    "        )\n",
    "\n",
    "\n",
    "# Helper functions to reduce repeated logic\n",
    "def init_metrics(optimization_metric=None):\n",
    "    \"\"\"Initialize metrics based on the optimization metric or all metrics.\"\"\"\n",
    "    if optimization_metric:\n",
    "        # Only initialize the specific metric needed\n",
    "        metric_factory, metric_key = METRIC_CONFIGS[optimization_metric]\n",
    "        # Create the metric by calling the lambda\n",
    "        metric_obj = metric_factory()\n",
    "        return [metric_obj], metric_key\n",
    "    else:\n",
    "        # Create all metrics from the config\n",
    "        return [factory() for factory, _ in METRIC_CONFIGS.values()], None\n",
    "\n",
    "\n",
    "def try_prediction_with_config(\n",
    "    model,\n",
    "    prediction_length,\n",
    "    context_length,\n",
    "    mode,\n",
    "    num_samples,\n",
    "    test_data,\n",
    "    freq,\n",
    "    use_kv_cache,\n",
    "    metrics,\n",
    "    min_context_length=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Attempt prediction with a specific configuration, handling OOM errors.\n",
    "\n",
    "    Args:\n",
    "        model: The loaded model instance to use\n",
    "        prediction_length: Prediction horizon length\n",
    "        context_length: Context window length\n",
    "        mode: Prediction mode\n",
    "        num_samples: Number of samples to generate (fixed for evaluation)\n",
    "        test_data: data to evaluate on\n",
    "        freq: frequency of the data\n",
    "        use_kv_cache: Whether to use key-value caching\n",
    "        metrics: Metrics to evaluate\n",
    "        min_context_length: Minimum allowed context length\n",
    "\n",
    "    Returns:\n",
    "        Metrics result if successful, None if OOM occurs and can't be resolved\n",
    "    \"\"\"\n",
    "    # Get patch size if min_context_length not provided\n",
    "    if min_context_length is None:\n",
    "        min_context_length = model.model.patch_embed.stride\n",
    "\n",
    "    # Ensure context_length is not smaller than the minimum\n",
    "    context_length = max(context_length, min_context_length)\n",
    "\n",
    "    # Use the TOTOModelPredictorWrapper\n",
    "    predictor_wrapper = TOTOModelPredictorWrapper(\n",
    "        model=model,\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "        mode=mode,\n",
    "        num_samples=num_samples,\n",
    "        use_kv_cache=use_kv_cache,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Attempt prediction and evaluation\n",
    "        res = evaluate_model(\n",
    "            predictor_wrapper,\n",
    "            test_data=test_data,\n",
    "            metrics=metrics,\n",
    "            axis=None,\n",
    "            batch_size=num_samples,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=get_seasonality(freq),\n",
    "        )\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_dataset_with_model(model, task: EvalTask) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a TOTO model on a gift-eval dataset.\n",
    "    Takes a pre-loaded model to avoid redundant model loading.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-loaded TOTO model\n",
    "        task: EvalTask containing all evaluation parameters\n",
    "    Returns:\n",
    "        DataFrame containing evaluation results\n",
    "    \"\"\"\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.manual_seed(task.seed)\n",
    "    np.random.seed(task.seed)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    print(f\"Evaluating dataset {task.dataset_name}, term={task.term}\")\n",
    "\n",
    "    # Initialize dataset\n",
    "    dataset = GiftEvalDataset(\n",
    "        name=task.dataset_name,\n",
    "        term=task.term,\n",
    "        to_univariate=False,\n",
    "        storage_env_var=\"GIFT_EVAL\",\n",
    "    )\n",
    "\n",
    "    # Get min context length from model\n",
    "    min_context_length = model.model.patch_embed.stride\n",
    "    print(f\"Model min context length (patch size): {min_context_length}\")\n",
    "\n",
    "    # Check if we're evaluating on validation data - context length search is only allowed for test data\n",
    "    is_validation_target = task.evaluation_target == \"validation\"\n",
    "\n",
    "    # Simply use the already prettified dataset key with frequency and term\n",
    "    ds_config = f\"{task.dataset_key}/{task.dataset_frequency}/{task.term}\"\n",
    "\n",
    "    if not task.pad_short_series:\n",
    "        context_length = min(\n",
    "            DEFAULT_CONTEXT_LENGTH, get_maximal_context_length(dataset)\n",
    "        )\n",
    "    else:\n",
    "        context_length = DEFAULT_CONTEXT_LENGTH\n",
    "\n",
    "    # Set up evaluation metrics - create all metrics from the config\n",
    "    metrics, _ = init_metrics()\n",
    "\n",
    "    # Calculate optimal batch size based on available GPU memory, not used for prediction\n",
    "    suggested_batch_size = calculate_optimal_batch_size(\n",
    "        model=model,\n",
    "        target_dim=dataset.target_dim,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "        context_length=context_length,\n",
    "        use_kv_cache=task.use_kv_cache,\n",
    "        num_samples=task.num_samples,\n",
    "    )\n",
    "\n",
    "    if is_validation_target:\n",
    "        # When evaluating on validation data, prepare that dataset\n",
    "        eval_data = prepare_evaluation_data(\n",
    "            dataset=dataset,\n",
    "            base_dataset=dataset.validation_dataset,\n",
    "            prediction_length=dataset.prediction_length,\n",
    "        )\n",
    "    else:\n",
    "        # When evaluating on test data, use the test data directly\n",
    "        eval_data = dataset.test_data\n",
    "\n",
    "    # Try prediction with the optimal parameters - pass loaded model directly\n",
    "    res = try_prediction_with_config(\n",
    "        model=model,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "        context_length=context_length,\n",
    "        mode=Multivariate(batch_size=suggested_batch_size),\n",
    "        num_samples=task.num_samples,\n",
    "        test_data=eval_data,\n",
    "        freq=dataset.freq,\n",
    "        use_kv_cache=task.use_kv_cache,\n",
    "        metrics=metrics,\n",
    "        min_context_length=min_context_length,\n",
    "    )\n",
    "\n",
    "    # Process results - check if prediction was successful\n",
    "    if res is None:\n",
    "        print(f\"Prediction failed for {ds_config}\")\n",
    "        # Return a DataFrame with just metadata but NaN for metrics\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"dataset\": [ds_config],\n",
    "                \"model\": [task.checkpoint_path],\n",
    "                \"eval_metrics/MSE[mean]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MSE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MAE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MASE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MAPE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/sMAPE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MSIS\": [float(\"nan\")],\n",
    "                \"eval_metrics/RMSE[mean]\": [float(\"nan\")],\n",
    "                \"eval_metrics/NRMSE[mean]\": [float(\"nan\")],\n",
    "                \"eval_metrics/ND[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/mean_weighted_sum_quantile_loss\": [float(\"nan\")],\n",
    "                \"domain\": [task.dataset_properties_map[task.dataset_key][\"domain\"]],\n",
    "                \"num_variates\": [\n",
    "                    task.dataset_properties_map[task.dataset_key][\"num_variates\"]\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create result dataframe\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dataset\": [ds_config],\n",
    "            \"model\": [task.checkpoint_path],\n",
    "            \"eval_metrics/MSE[mean]\": [res[\"MSE[mean]\"][0]],\n",
    "            \"eval_metrics/MSE[0.5]\": [res[\"MSE[0.5]\"][0]],\n",
    "            \"eval_metrics/MAE[0.5]\": [res[\"MAE[0.5]\"][0]],\n",
    "            \"eval_metrics/MASE[0.5]\": [res[\"MASE[0.5]\"][0]],\n",
    "            \"eval_metrics/MAPE[0.5]\": [res[\"MAPE[0.5]\"][0]],\n",
    "            \"eval_metrics/sMAPE[0.5]\": [res[\"sMAPE[0.5]\"][0]],\n",
    "            \"eval_metrics/MSIS\": [res[\"MSIS\"][0]],\n",
    "            \"eval_metrics/RMSE[mean]\": [res[\"RMSE[mean]\"][0]],\n",
    "            \"eval_metrics/NRMSE[mean]\": [res[\"NRMSE[mean]\"][0]],\n",
    "            \"eval_metrics/ND[0.5]\": [res[\"ND[0.5]\"][0]],\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\": [\n",
    "                res[\"mean_weighted_sum_quantile_loss\"][0]\n",
    "            ],\n",
    "            \"domain\": [task.dataset_properties_map[task.dataset_key][\"domain\"]],\n",
    "            \"num_variates\": [\n",
    "                task.dataset_properties_map[task.dataset_key][\"num_variates\"]\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Completed evaluation for {ds_config}\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def evaluate_tasks(tasks: List[EvalTask]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a batch of tasks sequentially, possibly from different checkpoints.\n",
    "    This function will load models on-demand.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    model = Toto.from_pretrained(\"Datadog/Toto-Open-Base-1.0\")\n",
    "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.eval()\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    # Process all tasks for this checkpoint\n",
    "    for task in tasks:\n",
    "        print(f\"Evaluating {task.dataset_name}, term={task.term}\")\n",
    "        result_df = evaluate_dataset_with_model(model, task)\n",
    "\n",
    "        if result_df is not None:\n",
    "            results.append(result_df)\n",
    "\n",
    "    # Cleanup model and memory only after completing all tasks\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if not results:\n",
    "        print(\"No successful evaluations in this task batch\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "<div style=\"background-color: #f2ecfc; padding: 10px; border-radius: 5px; font-family: Helvetica, Arial, sans-serif;\">\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/toto` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:13.841309Z",
     "iopub.status.busy": "2025-05-15T19:21:13.840928Z",
     "iopub.status.idle": "2025-05-15T19:21:26.103442Z",
     "shell.execute_reply": "2025-05-15T19:21:26.102339Z",
     "shell.execute_reply.started": "2025-05-15T19:21:13.841286Z"
    },
    "frozen": false,
    "scrolled": true,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GiftEval Benchmark\n",
      "Processing 4 tasks sequentially\n",
      "Loading weights from local directory\n",
      "Evaluating m4_weekly, term=short\n",
      "Evaluating dataset m4_weekly, term=short\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:04<00:00,  5.52it/s]\n",
      "359it [00:00, 667.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for m4_weekly/W/short\n",
      "Evaluating bizitobs_l2c/H, term=short\n",
      "Evaluating dataset bizitobs_l2c/H, term=short\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.22it/s]\n",
      "6it [00:00, 274.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for bizitobs_l2c/H/short\n",
      "Evaluating bizitobs_l2c/H, term=medium\n",
      "Evaluating dataset bizitobs_l2c/H, term=medium\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.34it/s]\n",
      "1it [00:00, 62.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for bizitobs_l2c/H/medium\n",
      "Evaluating bizitobs_l2c/H, term=long\n",
      "Evaluating dataset bizitobs_l2c/H, term=long\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.10it/s]\n",
      "1it [00:00, 55.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for bizitobs_l2c/H/long\n"
     ]
    }
   ],
   "source": [
    "# Inference parameters\n",
    "num_samples = 256\n",
    "use_kv_cache = True\n",
    "seed = 42\n",
    "evaluation_target = \"test\"\n",
    "pad_short_series = False\n",
    "dataset_groups = \"all\"\n",
    "\n",
    "print(f\"Evaluating GiftEval Benchmark\")\n",
    "\n",
    "# Load dataset properties\n",
    "dataset_properties_map = json.load(open(DATASET_PROPERTIES_PATH, \"r\"))\n",
    "# Get datasets based on selected group\n",
    "if dataset_groups == \"short\":\n",
    "    all_datasets = SHORT_DATASETS.split()\n",
    "    terms = [\"short\"]\n",
    "elif dataset_groups == \"med-long\":\n",
    "    all_datasets = MED_LONG_DATASETS.split()\n",
    "    terms = [\"medium\", \"long\"]\n",
    "else:  # \"all\"\n",
    "    all_datasets = list(set(SHORT_DATASETS.split() + MED_LONG_DATASETS.split()))\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "\n",
    "med_long_datasets = MED_LONG_DATASETS.split()\n",
    "\n",
    "# Create all tasks as a flat list\n",
    "all_tasks = []\n",
    "for dataset_name in all_datasets:\n",
    "    # Extract the dataset key and frequency\n",
    "    if \"/\" in dataset_name:\n",
    "        ds_key = dataset_name.split(\"/\")[0]\n",
    "        ds_freq = dataset_name.split(\"/\")[1]\n",
    "        ds_key = ds_key.lower()\n",
    "        ds_key = PRETTY_DATASET_NAMES.get(ds_key, ds_key)\n",
    "    else:\n",
    "        ds_key = dataset_name.lower()\n",
    "        ds_key = PRETTY_DATASET_NAMES.get(ds_key, ds_key)\n",
    "        ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "\n",
    "    for term in terms:\n",
    "        # Skip medium and long terms for datasets not in med_long_datasets\n",
    "        if (\n",
    "            term == \"medium\" or term == \"long\"\n",
    "        ) and dataset_name not in med_long_datasets:\n",
    "            continue\n",
    "\n",
    "        task = EvalTask(\n",
    "            dataset_name=dataset_name,\n",
    "            term=term,\n",
    "            checkpoint_path=\"Toto-Open-Base-1.0\",\n",
    "            num_samples=num_samples,\n",
    "            use_kv_cache=use_kv_cache,\n",
    "            seed=seed,\n",
    "            dataset_properties_map=dataset_properties_map,\n",
    "            dataset_key=ds_key,\n",
    "            dataset_frequency=ds_freq,\n",
    "            evaluation_target=evaluation_target,\n",
    "            pad_short_series=pad_short_series,\n",
    "        )\n",
    "\n",
    "        all_tasks.append(task)\n",
    "\n",
    "print(f\"Processing {len(all_tasks)} tasks sequentially\")\n",
    "\n",
    "# Process all tasks sequentially\n",
    "results = evaluate_tasks(all_tasks)\n",
    "\n",
    "results_filename = \"all_results\"\n",
    "\n",
    "results.to_csv(f\"../../results/gift_eval/toto/{results_filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "<div style=\"background-color: #f2ecfc; padding: 10px; border-radius: 5px; font-family: Helvetica, Arial, sans-serif;\">\n",
    "\n",
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/Toto` folder containing the results for the Toto model on the gift-eval benchmark. The csv file will look like this:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:59.477847Z",
     "iopub.status.busy": "2025-05-15T19:21:59.477467Z",
     "iopub.status.idle": "2025-05-15T19:21:59.544471Z",
     "shell.execute_reply": "2025-05-15T19:21:59.543779Z",
     "shell.execute_reply.started": "2025-05-15T19:21:59.477822Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>eval_metrics/MSE[mean]</th>\n",
       "      <th>eval_metrics/MSE[0.5]</th>\n",
       "      <th>eval_metrics/MAE[0.5]</th>\n",
       "      <th>eval_metrics/MASE[0.5]</th>\n",
       "      <th>eval_metrics/MAPE[0.5]</th>\n",
       "      <th>eval_metrics/sMAPE[0.5]</th>\n",
       "      <th>eval_metrics/MSIS</th>\n",
       "      <th>eval_metrics/RMSE[mean]</th>\n",
       "      <th>eval_metrics/NRMSE[mean]</th>\n",
       "      <th>eval_metrics/ND[0.5]</th>\n",
       "      <th>eval_metrics/mean_weighted_sum_quantile_loss</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_variates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m4_weekly/W/short</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>1.183982e+06</td>\n",
       "      <td>477587.829441</td>\n",
       "      <td>339.140669</td>\n",
       "      <td>2.399578</td>\n",
       "      <td>0.085594</td>\n",
       "      <td>0.092320</td>\n",
       "      <td>20.038562</td>\n",
       "      <td>1088.109550</td>\n",
       "      <td>0.198237</td>\n",
       "      <td>0.061786</td>\n",
       "      <td>0.049082</td>\n",
       "      <td>Econ/Fin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bizitobs_l2c/H/short</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>1.594931e+02</td>\n",
       "      <td>60.777010</td>\n",
       "      <td>4.745321</td>\n",
       "      <td>0.469607</td>\n",
       "      <td>0.432474</td>\n",
       "      <td>0.647721</td>\n",
       "      <td>3.146161</td>\n",
       "      <td>12.629057</td>\n",
       "      <td>0.680739</td>\n",
       "      <td>0.255785</td>\n",
       "      <td>0.198957</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bizitobs_l2c/H/medium</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>3.739842e+04</td>\n",
       "      <td>157.572898</td>\n",
       "      <td>7.473105</td>\n",
       "      <td>0.756662</td>\n",
       "      <td>0.579523</td>\n",
       "      <td>0.966016</td>\n",
       "      <td>6.472265</td>\n",
       "      <td>193.386702</td>\n",
       "      <td>11.709859</td>\n",
       "      <td>0.452508</td>\n",
       "      <td>0.356206</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bizitobs_l2c/H/long</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>4.352408e+04</td>\n",
       "      <td>138.808408</td>\n",
       "      <td>7.564872</td>\n",
       "      <td>0.797159</td>\n",
       "      <td>0.751385</td>\n",
       "      <td>0.940042</td>\n",
       "      <td>8.978071</td>\n",
       "      <td>208.624246</td>\n",
       "      <td>12.743371</td>\n",
       "      <td>0.462084</td>\n",
       "      <td>0.368572</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dataset               model  eval_metrics/MSE[mean]   \n",
       "0      m4_weekly/W/short  Toto-Open-Base-1.0            1.183982e+06  \\\n",
       "1   bizitobs_l2c/H/short  Toto-Open-Base-1.0            1.594931e+02   \n",
       "2  bizitobs_l2c/H/medium  Toto-Open-Base-1.0            3.739842e+04   \n",
       "3    bizitobs_l2c/H/long  Toto-Open-Base-1.0            4.352408e+04   \n",
       "\n",
       "   eval_metrics/MSE[0.5]  eval_metrics/MAE[0.5]  eval_metrics/MASE[0.5]   \n",
       "0          477587.829441             339.140669                2.399578  \\\n",
       "1              60.777010               4.745321                0.469607   \n",
       "2             157.572898               7.473105                0.756662   \n",
       "3             138.808408               7.564872                0.797159   \n",
       "\n",
       "   eval_metrics/MAPE[0.5]  eval_metrics/sMAPE[0.5]  eval_metrics/MSIS   \n",
       "0                0.085594                 0.092320          20.038562  \\\n",
       "1                0.432474                 0.647721           3.146161   \n",
       "2                0.579523                 0.966016           6.472265   \n",
       "3                0.751385                 0.940042           8.978071   \n",
       "\n",
       "   eval_metrics/RMSE[mean]  eval_metrics/NRMSE[mean]  eval_metrics/ND[0.5]   \n",
       "0              1088.109550                  0.198237              0.061786  \\\n",
       "1                12.629057                  0.680739              0.255785   \n",
       "2               193.386702                 11.709859              0.452508   \n",
       "3               208.624246                 12.743371              0.462084   \n",
       "\n",
       "   eval_metrics/mean_weighted_sum_quantile_loss        domain  num_variates  \n",
       "0                                      0.049082      Econ/Fin             1  \n",
       "1                                      0.198957  Web/CloudOps             7  \n",
       "2                                      0.356206  Web/CloudOps             7  \n",
       "3                                      0.368572  Web/CloudOps             7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../../results/gift_eval/toto/{results_filename}.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "dd-sharing": {
   "allowed_groups": [
    "subproduct-datascience",
    "combined-data-science",
    "team-largemodelfoundationsresearch",
    ""
   ],
   "allowed_users": [
    ""
   ],
   "retention_period": "90"
  },
  "kernelspec": {
   "display_name": "toto_test_env",
   "language": "python",
   "name": "toto-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
