{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "# Quick Start: Running Foundation Model Toto on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run the Toto on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:20:55.692738Z",
     "iopub.status.busy": "2025-05-15T19:20:55.692325Z",
     "iopub.status.idle": "2025-05-15T19:20:55.710224Z",
     "shell.execute_reply": "2025-05-15T19:20:55.709727Z",
     "shell.execute_reply.started": "2025-05-15T19:20:55.692710Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:20:57.277581Z",
     "iopub.status.busy": "2025-05-15T19:20:57.277225Z",
     "iopub.status.idle": "2025-05-15T19:21:03.504890Z",
     "shell.execute_reply": "2025-05-15T19:21:03.504262Z",
     "shell.execute_reply.started": "2025-05-15T19:20:57.277556Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/venvs/toto-test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "os.environ[\"GIFT_EVAL\"] = \"Change/To/GiftEval/Local/Path\"\n",
    "\n",
    "# Standard library imports\n",
    "import argparse\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "# Third-party imports\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import urllib.parse\n",
    "from dataclasses import dataclass\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "# Local imports\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from scipy import stats\n",
    "\n",
    "from gift_eval.data import Dataset as GiftEvalDataset\n",
    "\n",
    "from inference.gluonts_predictor import Multivariate, TotoPredictor\n",
    "from model.toto import Toto\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from gluonts.ev.aggregations import Mean\n",
    "from gluonts.ev.metrics import BaseMetricDefinition, DirectMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:03.506259Z",
     "iopub.status.busy": "2025-05-15T19:21:03.505819Z",
     "iopub.status.idle": "2025-05-15T19:21:03.549366Z",
     "shell.execute_reply": "2025-05-15T19:21:03.548870Z",
     "shell.execute_reply.started": "2025-05-15T19:21:03.506236Z"
    },
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET_PROPERTIES_PATH = \"./dataset_properties.json\"\n",
    "\n",
    "DEFAULT_CONTEXT_LENGTH = 4096\n",
    "\n",
    "PRETTY_DATASET_NAMES = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "# SHORT_DATASETS = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "# MED_LONG_DATASETS = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "SHORT_DATASETS = \"m4_weekly\"\n",
    "MED_LONG_DATASETS = \"bizitobs_l2c/H\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:03.550190Z",
     "iopub.status.busy": "2025-05-15T19:21:03.549940Z",
     "iopub.status.idle": "2025-05-15T19:21:03.594769Z",
     "shell.execute_reply": "2025-05-15T19:21:03.594243Z",
     "shell.execute_reply.started": "2025-05-15T19:21:03.550172Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Define metrics configuration once at module level\n",
    "METRIC_CONFIGS = {\n",
    "    \"MAE\": (lambda: MAE(), \"MAE[0.5]\"),\n",
    "    \"MSE\": (lambda: MSE(forecast_type=0.5), \"MSE[0.5]\"),\n",
    "    \"MSE_MEAN\": (lambda: MSE(forecast_type=\"mean\"), \"MSE[mean]\"),\n",
    "    \"MASE\": (lambda: MASE(), \"MASE[0.5]\"),\n",
    "    \"MAPE\": (lambda: MAPE(), \"MAPE[0.5]\"),\n",
    "    \"SMAPE\": (lambda: SMAPE(), \"sMAPE[0.5]\"),\n",
    "    \"MSIS\": (lambda: MSIS(), \"MSIS\"),\n",
    "    \"RMSE\": (lambda: RMSE(forecast_type=0.5), \"RMSE[0.5]\"),\n",
    "    \"RMSE_MEAN\": (lambda: RMSE(forecast_type=\"mean\"), \"RMSE[mean]\"),\n",
    "    \"NRMSE\": (lambda: NRMSE(forecast_type=0.5), \"NRMSE[0.5]\"),\n",
    "    \"NRMSE_MEAN\": (lambda: NRMSE(forecast_type=\"mean\"), \"NRMSE[mean]\"),\n",
    "    \"ND\": (lambda: ND(), \"ND[0.5]\"),\n",
    "    \"WQTL\": (\n",
    "        lambda: MeanWeightedSumQuantileLoss(quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "        \"mean_weighted_sum_quantile_loss\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Toto Predictor\n",
    "Load Toto model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:04.644703Z",
     "iopub.status.busy": "2025-05-15T19:21:04.644339Z",
     "iopub.status.idle": "2025-05-15T19:21:04.720992Z",
     "shell.execute_reply": "2025-05-15T19:21:04.720430Z",
     "shell.execute_reply.started": "2025-05-15T19:21:04.644679Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class EvalTask:\n",
    "    \"\"\"Dataclass representing an evaluation task with all necessary parameters.\"\"\"\n",
    "\n",
    "    dataset_name: str\n",
    "    term: str\n",
    "    checkpoint_path: str\n",
    "    num_samples: int\n",
    "    use_kv_cache: bool\n",
    "    seed: int\n",
    "    dataset_properties_map: Dict[str, Any]\n",
    "    dataset_frequency: str\n",
    "    dataset_key: str\n",
    "    evaluation_target: str = \"test\"  # Can be \"test\" or \"validation\"\n",
    "    pad_short_series: bool = False\n",
    "\n",
    "\n",
    "def get_total_gpu_memory():\n",
    "    \"\"\"Get total GPU VRAM capacity in MB.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.cuda.current_device()\n",
    "    return torch.cuda.get_device_properties(device).total_memory / (1024 * 1024)\n",
    "\n",
    "\n",
    "def calculate_optimal_batch_size(\n",
    "    model, target_dim, prediction_length, context_length, use_kv_cache, num_samples, safety_factor=0.01\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate the optimal batch size based on available GPU memory and model requirements.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-loaded TOTO model\n",
    "        target_dim: Target dimensionality (number of variates)\n",
    "        prediction_length: Length of prediction horizon\n",
    "        context_length: Context window length\n",
    "        use_kv_cache: Whether KV cache is used\n",
    "        num_samples: Number of samples to generate\n",
    "        safety_factor: Safety factor to apply when calculating available memory (default=0.01)\n",
    "\n",
    "    Returns:\n",
    "        Suggested batch size for prediction\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Extract model size information\n",
    "        model_width = model.model.embed_dim  # Feature dimension\n",
    "        model_depth = model.model.num_layers  # Number of transformer layers\n",
    "\n",
    "        # Calculate model's parameter memory footprint in MB\n",
    "        model_param_memory_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "\n",
    "        # Base memory per sample in MB (parameters + activations + gradients)\n",
    "        base_memory_per_sample = (model_width * model_depth * 4) / (1024 * 1024)\n",
    "\n",
    "        # Memory for input/output tensors\n",
    "        io_memory = (target_dim * (context_length + prediction_length) * 4) / (1024 * 1024)\n",
    "\n",
    "        # KV cache memory (if used)\n",
    "        kv_memory = 0\n",
    "        if use_kv_cache:\n",
    "            kv_memory = (model_depth * model_width * 2 * context_length * 4) / (1024 * 1024)\n",
    "\n",
    "        # Total memory per sample\n",
    "        mem_per_sample_mb = base_memory_per_sample + io_memory + kv_memory\n",
    "\n",
    "        # Factor in target dimensions and samples directly\n",
    "        # Each dimension and sample has a direct multiplicative effect on memory\n",
    "        mem_per_batch_mb = (\n",
    "            mem_per_sample_mb * target_dim * num_samples\n",
    "        )  # Total memory for a batch with num_samples samples\n",
    "\n",
    "        # Get total GPU VRAM capacity and subtract model parameter memory\n",
    "        gpu_mem = get_total_gpu_memory()  # in MB\n",
    "        cuda_reserved_mb = 1024  # Reserve 1GB for CUDA runtime and other overhead\n",
    "\n",
    "        # Available memory = (Total VRAM - Model parameters - CUDA reserved) * safety factor\n",
    "        available_memory = (gpu_mem - model_param_memory_mb - cuda_reserved_mb) * safety_factor\n",
    "\n",
    "        # Calculate max batch size based on available memory\n",
    "        max_batch_size = max(1, int(available_memory / (mem_per_batch_mb / num_samples)))\n",
    "    \n",
    "        max_batch_size = min(16, max_batch_size)\n",
    "        return max_batch_size\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error calculating optimal batch size: {e}\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_maximal_context_length(dataset: GiftEvalDataset):\n",
    "    \"\"\"\n",
    "    Calculates the maximal context length that can be used for the given dataset,\n",
    "    based on the shortest time series in the dataset and the number of\n",
    "    prediction windows used for validation and testing.\n",
    "\n",
    "    The context length is computed by subtracting the total number of\n",
    "    prediction steps (across test and val windows) from the shortest\n",
    "    time series length in the dataset.\n",
    "    \"\"\"\n",
    "    shortest_series_in_dataset = dataset._min_series_length\n",
    "    total_prediction_windows = (\n",
    "        dataset.windows + 1\n",
    "    )  # dataset.windows is the number of windows in the rolling evaluation in the test split, and 1 is the prediction window we leave out in the validation split -> everything else before can be used as context data\n",
    "    max_context_length = (\n",
    "        shortest_series_in_dataset - total_prediction_windows * dataset.prediction_length\n",
    "    )  # total series length - (number of eval windows + 1) * eval window length\n",
    "    return max_context_length\n",
    "\n",
    "\n",
    "def prepare_evaluation_data(dataset: GiftEvalDataset, base_dataset, prediction_length: int):\n",
    "    \"\"\"\n",
    "    Helper function to prepare evaluation data by splitting a dataset and generating instances.\n",
    "\n",
    "    Args:\n",
    "        dataset: The GiftEvalDataset instance containing dataset metadata\n",
    "        base_dataset: The base dataset to split (training or validation dataset)\n",
    "        prediction_length: The prediction horizon length\n",
    "\n",
    "    Returns:\n",
    "        Generated evaluation data ready for model evaluation\n",
    "    \"\"\"\n",
    "    # Determine the number of validation windows based on dataset type\n",
    "    if \"m4\" in dataset.name:\n",
    "        # Special case for M4 datasets\n",
    "        validation_windows = 1\n",
    "        print(f\"M4 dataset detected: using {validation_windows} window\")\n",
    "    else:\n",
    "        # Use the same windows count as in the dataset\n",
    "        validation_windows = dataset.windows\n",
    "        print(f\"Using dataset.windows = {validation_windows} windows for evaluation\")\n",
    "\n",
    "    # Split the dataset and create evaluation instances\n",
    "    _, test_template = split(base_dataset, offset=-prediction_length * dataset.windows)\n",
    "\n",
    "    # Generate instances for evaluation\n",
    "    evaluation_data = test_template.generate_instances(\n",
    "        prediction_length=prediction_length,\n",
    "        windows=validation_windows,\n",
    "        distance=prediction_length,\n",
    "    )\n",
    "\n",
    "    return evaluation_data\n",
    "\n",
    "\n",
    "class TOTOModelPredictorWrapper:\n",
    "    \"\"\"Wrapper for TOTOPredictor that handles OOM errors by adjusting batch size.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        prediction_length,\n",
    "        context_length,\n",
    "        mode,\n",
    "        num_samples=128,\n",
    "        use_kv_cache=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the predictor wrapper with specified parameters.\n",
    "\n",
    "        Args:\n",
    "            model: The loaded TOTO model instance to use for predictions\n",
    "            prediction_length: The length of the prediction horizon.\n",
    "            context_length: The length of the context window.\n",
    "            mode: Mode of prediction (e.g., \"forecast\").\n",
    "            num_samples: Total number of samples to generate.\n",
    "            use_kv_cache: Whether to use key-value caching.\n",
    "        \"\"\"\n",
    "\n",
    "        self.prediction_length = prediction_length\n",
    "        self.context_length = context_length\n",
    "        self.mode = mode\n",
    "        self.num_samples = num_samples\n",
    "        self.use_kv_cache = use_kv_cache\n",
    "        self.samples_per_batch = num_samples  # Start with full batch size and adjust if needed\n",
    "        self.model = model\n",
    "        self._adjusted = False  # Tracks whether adjustment has been done\n",
    "\n",
    "        self._initialize_predictor()\n",
    "\n",
    "    def _initialize_predictor(self):\n",
    "        \"\"\"\n",
    "        Initialize the TOTOPredictor with the current samples_per_batch.\n",
    "        \"\"\"\n",
    "        self.predictor = TotoPredictor.create_for_eval(\n",
    "            model=self.model,\n",
    "            prediction_length=self.prediction_length,\n",
    "            context_length=self.context_length,\n",
    "            mode=self.mode,\n",
    "            samples_per_batch=self.samples_per_batch,\n",
    "        )\n",
    "\n",
    "    def predict(self, gluonts_test_data: tuple):\n",
    "        \"\"\"\n",
    "        Perform prediction while adjusting num_samples, samples_per_batch, and context_length if OOM errors occur.\n",
    "        \"\"\"\n",
    "        predictions = None\n",
    "\n",
    "        # Adjust only on the first call.\n",
    "        if not self._adjusted:\n",
    "\n",
    "            print(\"Initializing predictor with samples_per_batch =\", self.samples_per_batch)\n",
    "            while self.samples_per_batch >= 1:\n",
    "                try:\n",
    "                    print(\n",
    "                        f\"Attempting prediction with samples_per_batch = {self.samples_per_batch} and context_length = {self.context_length}\"\n",
    "                    )\n",
    "                    # Attempt prediction (consume the generator to catch any OOM)\n",
    "                    predictions = list(\n",
    "                        self.predictor.predict(\n",
    "                            gluonts_test_data,\n",
    "                            use_kv_cache=self.use_kv_cache,\n",
    "                            num_samples=self.num_samples,\n",
    "                        )\n",
    "                    )\n",
    "                    self._adjusted = True\n",
    "                    return predictions  # Prediction succeeded\n",
    "\n",
    "                except RuntimeError as e:\n",
    "                    print(\"Caught exception during prediction:\", e)\n",
    "                    if \"CUDA out of memory\" in str(e):\n",
    "                        # First, try reducing the batch size if possible.\n",
    "                        if self.samples_per_batch > 1:\n",
    "                            print(\n",
    "                                f\"Out of memory with samples_per_batch = {self.samples_per_batch}. Reducing batch size.\"\n",
    "                            )\n",
    "                            self.samples_per_batch = self.samples_per_batch // 2\n",
    "                            # Clean up GPU memory before trying with smaller batch size\n",
    "                            torch.cuda.empty_cache()\n",
    "                        else:\n",
    "                            # Cannot reduce batch size further, so we fail\n",
    "                            print(\n",
    "                                f\"OOM at minimal batch size. Cannot proceed with this context length and sample count.\"\n",
    "                            )\n",
    "                            raise e\n",
    "                        # Reinitialize the predictor with the new settings.\n",
    "                        self._initialize_predictor()\n",
    "                    else:\n",
    "                        raise e  # Re-raise unexpected exceptions\n",
    "\n",
    "        # For subsequent calls, simply return the generator.\n",
    "        return self.predictor.predict(gluonts_test_data, use_kv_cache=self.use_kv_cache, num_samples=self.num_samples)\n",
    "\n",
    "\n",
    "# Helper functions to reduce repeated logic\n",
    "def init_metrics(optimization_metric=None):\n",
    "    \"\"\"Initialize metrics based on the optimization metric or all metrics.\"\"\"\n",
    "    if optimization_metric:\n",
    "        # Only initialize the specific metric needed\n",
    "        metric_factory, metric_key = METRIC_CONFIGS[optimization_metric]\n",
    "        # Create the metric by calling the lambda\n",
    "        metric_obj = metric_factory()\n",
    "        return [metric_obj], metric_key\n",
    "    else:\n",
    "        # Create all metrics from the config\n",
    "        return [factory() for factory, _ in METRIC_CONFIGS.values()], None\n",
    "\n",
    "\n",
    "def try_prediction_with_config(\n",
    "    model,\n",
    "    prediction_length,\n",
    "    context_length,\n",
    "    mode,\n",
    "    num_samples,\n",
    "    test_data,\n",
    "    freq,\n",
    "    use_kv_cache,\n",
    "    metrics,\n",
    "    min_context_length=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Attempt prediction with a specific configuration, handling OOM errors.\n",
    "\n",
    "    Args:\n",
    "        model: The loaded model instance to use\n",
    "        prediction_length: Prediction horizon length\n",
    "        context_length: Context window length\n",
    "        mode: Prediction mode\n",
    "        num_samples: Number of samples to generate (fixed for evaluation)\n",
    "        test_data: data to evaluate on\n",
    "        freq: frequency of the data\n",
    "        use_kv_cache: Whether to use key-value caching\n",
    "        metrics: Metrics to evaluate\n",
    "        min_context_length: Minimum allowed context length\n",
    "\n",
    "    Returns:\n",
    "        Metrics result if successful, None if OOM occurs and can't be resolved\n",
    "    \"\"\"\n",
    "    # Get patch size if min_context_length not provided\n",
    "    if min_context_length is None:\n",
    "        min_context_length = model.model.patch_embed.stride\n",
    "\n",
    "    # Ensure context_length is not smaller than the minimum\n",
    "    context_length = max(context_length, min_context_length)\n",
    "\n",
    "    # Use the TOTOModelPredictorWrapper\n",
    "    predictor_wrapper = TOTOModelPredictorWrapper(\n",
    "        model=model,\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "        mode=mode,\n",
    "        num_samples=num_samples,\n",
    "        use_kv_cache=use_kv_cache,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Attempt prediction and evaluation\n",
    "        res = evaluate_model(\n",
    "            predictor_wrapper,\n",
    "            test_data=test_data,\n",
    "            metrics=metrics,\n",
    "            axis=None,\n",
    "            batch_size=num_samples,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=get_seasonality(freq),\n",
    "        )\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_dataset_with_model(\n",
    "    model, task: EvalTask\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a TOTO model on a gift-eval dataset.\n",
    "    Takes a pre-loaded model to avoid redundant model loading.\n",
    "\n",
    "    Args:\n",
    "        model: Pre-loaded TOTO model\n",
    "        task: EvalTask containing all evaluation parameters\n",
    "    Returns:\n",
    "        DataFrame containing evaluation results\n",
    "    \"\"\"\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.manual_seed(task.seed)\n",
    "    np.random.seed(task.seed)\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    print(f\"Evaluating dataset {task.dataset_name}, term={task.term}\")\n",
    "\n",
    "    # Initialize dataset\n",
    "    dataset = GiftEvalDataset(\n",
    "        name=task.dataset_name,\n",
    "        term=task.term,\n",
    "        to_univariate=False,\n",
    "        storage_env_var=\"GIFT_EVAL\",\n",
    "    )\n",
    "\n",
    "    # Get min context length from model\n",
    "    min_context_length = model.model.patch_embed.stride\n",
    "    print(f\"Model min context length (patch size): {min_context_length}\")\n",
    "\n",
    "    # Check if we're evaluating on validation data - context length search is only allowed for test data\n",
    "    is_validation_target = task.evaluation_target == \"validation\"\n",
    "\n",
    "    # Simply use the already prettified dataset key with frequency and term\n",
    "    ds_config = f\"{task.dataset_key}/{task.dataset_frequency}/{task.term}\"\n",
    "\n",
    "\n",
    "    if not task.pad_short_series:\n",
    "        context_length = min(DEFAULT_CONTEXT_LENGTH, get_maximal_context_length(dataset))\n",
    "    else:\n",
    "        context_length = DEFAULT_CONTEXT_LENGTH\n",
    "\n",
    "    # Set up evaluation metrics - create all metrics from the config\n",
    "    metrics, _ = init_metrics()\n",
    "\n",
    "    # Calculate optimal batch size\n",
    "    suggested_batch_size = calculate_optimal_batch_size(\n",
    "        model=model,\n",
    "        target_dim=dataset.target_dim,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "        context_length=context_length,\n",
    "        use_kv_cache=task.use_kv_cache,\n",
    "        num_samples=task.num_samples,\n",
    "    )\n",
    "\n",
    "    if is_validation_target:\n",
    "        # When evaluating on validation data, prepare that dataset\n",
    "        eval_data = prepare_evaluation_data(\n",
    "            dataset=dataset, base_dataset=dataset.validation_dataset, prediction_length=dataset.prediction_length\n",
    "        )\n",
    "    else:\n",
    "        # When evaluating on test data, use the test data directly\n",
    "        eval_data = dataset.test_data\n",
    "\n",
    "    # Try prediction with the optimal parameters - pass loaded model directly\n",
    "    res = try_prediction_with_config(\n",
    "        model=model,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "        context_length=context_length,\n",
    "        mode=Multivariate(batch_size=suggested_batch_size),\n",
    "        num_samples=task.num_samples,\n",
    "        test_data=eval_data,\n",
    "        freq=dataset.freq,\n",
    "        use_kv_cache=task.use_kv_cache,\n",
    "        metrics=metrics,\n",
    "        min_context_length=min_context_length,\n",
    "    )\n",
    "\n",
    "    # Process results - check if prediction was successful\n",
    "    if res is None:\n",
    "        print(f\"Prediction failed for {ds_config}\")\n",
    "        # Return a DataFrame with just metadata but NaN for metrics\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"dataset\": [ds_config],\n",
    "                \"model\": [task.checkpoint_path],\n",
    "                \"eval_metrics/MSE[mean]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MSE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MAE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MASE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MAPE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/sMAPE[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/MSIS\": [float(\"nan\")],\n",
    "                \"eval_metrics/RMSE[mean]\": [float(\"nan\")],\n",
    "                \"eval_metrics/NRMSE[mean]\": [float(\"nan\")],\n",
    "                \"eval_metrics/ND[0.5]\": [float(\"nan\")],\n",
    "                \"eval_metrics/mean_weighted_sum_quantile_loss\": [float(\"nan\")],\n",
    "                \"domain\": [task.dataset_properties_map[task.dataset_key][\"domain\"]],\n",
    "                \"num_variates\": [task.dataset_properties_map[task.dataset_key][\"num_variates\"]],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create result dataframe\n",
    "    result_df = pd.DataFrame(\n",
    "        {\n",
    "            \"dataset\": [ds_config],\n",
    "            \"model\": [task.checkpoint_path],\n",
    "            \"eval_metrics/MSE[mean]\": [res[\"MSE[mean]\"][0]],\n",
    "            \"eval_metrics/MSE[0.5]\": [res[\"MSE[0.5]\"][0]],\n",
    "            \"eval_metrics/MAE[0.5]\": [res[\"MAE[0.5]\"][0]],\n",
    "            \"eval_metrics/MASE[0.5]\": [res[\"MASE[0.5]\"][0]],\n",
    "            \"eval_metrics/MAPE[0.5]\": [res[\"MAPE[0.5]\"][0]],\n",
    "            \"eval_metrics/sMAPE[0.5]\": [res[\"sMAPE[0.5]\"][0]],\n",
    "            \"eval_metrics/MSIS\": [res[\"MSIS\"][0]],\n",
    "            \"eval_metrics/RMSE[mean]\": [res[\"RMSE[mean]\"][0]],\n",
    "            \"eval_metrics/NRMSE[mean]\": [res[\"NRMSE[mean]\"][0]],\n",
    "            \"eval_metrics/ND[0.5]\": [res[\"ND[0.5]\"][0]],\n",
    "            \"eval_metrics/mean_weighted_sum_quantile_loss\": [res[\"mean_weighted_sum_quantile_loss\"][0]],\n",
    "            \"domain\": [task.dataset_properties_map[task.dataset_key][\"domain\"]],\n",
    "            \"num_variates\": [task.dataset_properties_map[task.dataset_key][\"num_variates\"]],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Completed evaluation for {ds_config}\")\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def evaluate_tasks(tasks: List[EvalTask]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a batch of tasks sequentially, possibly from different checkpoints.\n",
    "    This function will load models on-demand.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "    model = Toto.from_pretrained('Datadog/Toto-Open-Base-1.0')\n",
    "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.eval()\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    # Process all tasks for this checkpoint\n",
    "    for task in tasks:\n",
    "        print(f\"Evaluating {task.dataset_name}, term={task.term}\")\n",
    "        result_df = evaluate_dataset_with_model(model, task)\n",
    "\n",
    "        if result_df is not None:\n",
    "            results.append(result_df)\n",
    "\n",
    "    # Cleanup model and memory only after completing all tasks\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if not results:\n",
    "        print(\"No successful evaluations in this task batch\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we have our predictor class, we can use it to predict on the gift-eval benchmark datasets. We will use the `evaluate_model` function to evaluate the model. This function is a helper function to evaluate the model on the test data and return the results in a dictionary. We are going to follow the naming conventions explained in the [README](../README.md) file to store the results in a csv file called `all_results.csv` under the `results/toto` folder.\n",
    "\n",
    "The first column in the csv file is the dataset config name which is a combination of the dataset name, frequency and the term:\n",
    "\n",
    "```python\n",
    "f\"{dataset_name}/{freq}/{term}\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:13.841309Z",
     "iopub.status.busy": "2025-05-15T19:21:13.840928Z",
     "iopub.status.idle": "2025-05-15T19:21:26.103442Z",
     "shell.execute_reply": "2025-05-15T19:21:26.102339Z",
     "shell.execute_reply.started": "2025-05-15T19:21:13.841286Z"
    },
    "frozen": false,
    "scrolled": true,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating GiftEval Benchmark\n",
      "Processing 4 tasks sequentially\n",
      "Loading weights from local directory\n",
      "Evaluating m4_weekly, term=short\n",
      "Evaluating dataset m4_weekly, term=short\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:04<00:00,  5.52it/s]\n",
      "359it [00:00, 667.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for m4_weekly/W/short\n",
      "Evaluating bizitobs_l2c/H, term=short\n",
      "Evaluating dataset bizitobs_l2c/H, term=short\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 2328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.22it/s]\n",
      "6it [00:00, 274.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for bizitobs_l2c/H/short\n",
      "Evaluating bizitobs_l2c/H, term=medium\n",
      "Evaluating dataset bizitobs_l2c/H, term=medium\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "1it [00:00, 62.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for bizitobs_l2c/H/medium\n",
      "Evaluating bizitobs_l2c/H, term=long\n",
      "Evaluating dataset bizitobs_l2c/H, term=long\n",
      "Model min context length (patch size): 64\n",
      "Initializing predictor with samples_per_batch = 256\n",
      "Attempting prediction with samples_per_batch = 256 and context_length = 1224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "1it [00:00, 55.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for bizitobs_l2c/H/long\n"
     ]
    }
   ],
   "source": [
    "# Inference parameters\n",
    "num_samples=256\n",
    "use_kv_cache=True\n",
    "seed=42\n",
    "evaluation_target=\"test\"\n",
    "pad_short_series=False\n",
    "dataset_groups=\"all\"\n",
    "\n",
    "print(f\"Evaluating GiftEval Benchmark\")\n",
    "\n",
    "# Load dataset properties\n",
    "dataset_properties_map = json.load(\n",
    "    open(DATASET_PROPERTIES_PATH, \"r\")\n",
    ")\n",
    "# Get datasets based on selected group\n",
    "if dataset_groups == \"short\":\n",
    "    all_datasets = SHORT_DATASETS.split()\n",
    "    terms = [\"short\"]\n",
    "elif dataset_groups == \"med-long\":\n",
    "    all_datasets = MED_LONG_DATASETS.split()\n",
    "    terms = [\"medium\", \"long\"]\n",
    "else:  # \"all\"\n",
    "    all_datasets = (\n",
    "        list(set(SHORT_DATASETS.split() + MED_LONG_DATASETS.split()))\n",
    "    )\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "\n",
    "med_long_datasets = MED_LONG_DATASETS.split()\n",
    "\n",
    "# Create all tasks as a flat list\n",
    "all_tasks = []\n",
    "for dataset_name in all_datasets:\n",
    "    # Extract the dataset key and frequency\n",
    "    if \"/\" in dataset_name:\n",
    "        ds_key = dataset_name.split(\"/\")[0]\n",
    "        ds_freq = dataset_name.split(\"/\")[1]\n",
    "        ds_key = ds_key.lower()\n",
    "        ds_key = PRETTY_DATASET_NAMES.get(ds_key, ds_key)\n",
    "    else:\n",
    "        ds_key = dataset_name.lower()\n",
    "        ds_key = PRETTY_DATASET_NAMES.get(ds_key, ds_key)\n",
    "        ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "\n",
    "    for term in terms:\n",
    "        # Skip medium and long terms for datasets not in med_long_datasets\n",
    "        if (term == \"medium\" or term == \"long\") and dataset_name not in med_long_datasets:\n",
    "            continue\n",
    "\n",
    "        task = EvalTask(\n",
    "            dataset_name=dataset_name,\n",
    "            term=term,\n",
    "            checkpoint_path=\"Toto-Open-Base-1.0\",\n",
    "            num_samples=num_samples,\n",
    "            use_kv_cache=use_kv_cache,\n",
    "            seed=seed,\n",
    "            dataset_properties_map=dataset_properties_map,\n",
    "            dataset_key=ds_key,\n",
    "            dataset_frequency=ds_freq,\n",
    "            evaluation_target=evaluation_target,\n",
    "            pad_short_series=pad_short_series,\n",
    "        )\n",
    "\n",
    "        all_tasks.append(task)\n",
    "\n",
    "print(f\"Processing {len(all_tasks)} tasks sequentially\")\n",
    "\n",
    "# Process all tasks sequentially\n",
    "results = evaluate_tasks(all_tasks)\n",
    "\n",
    "results_filename = \"Toto-Open-Base-1.0\"\n",
    "\n",
    "results.to_csv(f\"{results_filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "source": [
    "## Results\n",
    "\n",
    "Running the above cell will generate a csv file called `all_results.csv` under the `results/Toto` folder containing the results for the Toto model on the gift-eval benchmark. The csv file will look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T19:21:59.477847Z",
     "iopub.status.busy": "2025-05-15T19:21:59.477467Z",
     "iopub.status.idle": "2025-05-15T19:21:59.544471Z",
     "shell.execute_reply": "2025-05-15T19:21:59.543779Z",
     "shell.execute_reply.started": "2025-05-15T19:21:59.477822Z"
    },
    "frozen": false,
    "tags": [
     "unsafe_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>eval_metrics/MSE[mean]</th>\n",
       "      <th>eval_metrics/MSE[0.5]</th>\n",
       "      <th>eval_metrics/MAE[0.5]</th>\n",
       "      <th>eval_metrics/MASE[0.5]</th>\n",
       "      <th>eval_metrics/MAPE[0.5]</th>\n",
       "      <th>eval_metrics/sMAPE[0.5]</th>\n",
       "      <th>eval_metrics/MSIS</th>\n",
       "      <th>eval_metrics/RMSE[mean]</th>\n",
       "      <th>eval_metrics/NRMSE[mean]</th>\n",
       "      <th>eval_metrics/ND[0.5]</th>\n",
       "      <th>eval_metrics/mean_weighted_sum_quantile_loss</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_variates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m4_weekly/W/short</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>1.183982e+06</td>\n",
       "      <td>477587.829441</td>\n",
       "      <td>339.140669</td>\n",
       "      <td>2.399578</td>\n",
       "      <td>0.085594</td>\n",
       "      <td>0.092320</td>\n",
       "      <td>20.038562</td>\n",
       "      <td>1088.109550</td>\n",
       "      <td>0.198237</td>\n",
       "      <td>0.061786</td>\n",
       "      <td>0.049082</td>\n",
       "      <td>Econ/Fin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bizitobs_l2c/H/short</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>1.594931e+02</td>\n",
       "      <td>60.777010</td>\n",
       "      <td>4.745321</td>\n",
       "      <td>0.469607</td>\n",
       "      <td>0.432474</td>\n",
       "      <td>0.647721</td>\n",
       "      <td>3.146161</td>\n",
       "      <td>12.629057</td>\n",
       "      <td>0.680739</td>\n",
       "      <td>0.255785</td>\n",
       "      <td>0.198957</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bizitobs_l2c/H/medium</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>3.739842e+04</td>\n",
       "      <td>157.572898</td>\n",
       "      <td>7.473105</td>\n",
       "      <td>0.756662</td>\n",
       "      <td>0.579523</td>\n",
       "      <td>0.966016</td>\n",
       "      <td>6.472265</td>\n",
       "      <td>193.386702</td>\n",
       "      <td>11.709859</td>\n",
       "      <td>0.452508</td>\n",
       "      <td>0.356206</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bizitobs_l2c/H/long</td>\n",
       "      <td>Toto-Open-Base-1.0</td>\n",
       "      <td>4.352408e+04</td>\n",
       "      <td>138.808408</td>\n",
       "      <td>7.564872</td>\n",
       "      <td>0.797159</td>\n",
       "      <td>0.751385</td>\n",
       "      <td>0.940042</td>\n",
       "      <td>8.978071</td>\n",
       "      <td>208.624246</td>\n",
       "      <td>12.743371</td>\n",
       "      <td>0.462084</td>\n",
       "      <td>0.368572</td>\n",
       "      <td>Web/CloudOps</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 dataset               model  eval_metrics/MSE[mean]   \n",
       "0      m4_weekly/W/short  Toto-Open-Base-1.0            1.183982e+06  \\\n",
       "1   bizitobs_l2c/H/short  Toto-Open-Base-1.0            1.594931e+02   \n",
       "2  bizitobs_l2c/H/medium  Toto-Open-Base-1.0            3.739842e+04   \n",
       "3    bizitobs_l2c/H/long  Toto-Open-Base-1.0            4.352408e+04   \n",
       "\n",
       "   eval_metrics/MSE[0.5]  eval_metrics/MAE[0.5]  eval_metrics/MASE[0.5]   \n",
       "0          477587.829441             339.140669                2.399578  \\\n",
       "1              60.777010               4.745321                0.469607   \n",
       "2             157.572898               7.473105                0.756662   \n",
       "3             138.808408               7.564872                0.797159   \n",
       "\n",
       "   eval_metrics/MAPE[0.5]  eval_metrics/sMAPE[0.5]  eval_metrics/MSIS   \n",
       "0                0.085594                 0.092320          20.038562  \\\n",
       "1                0.432474                 0.647721           3.146161   \n",
       "2                0.579523                 0.966016           6.472265   \n",
       "3                0.751385                 0.940042           8.978071   \n",
       "\n",
       "   eval_metrics/RMSE[mean]  eval_metrics/NRMSE[mean]  eval_metrics/ND[0.5]   \n",
       "0              1088.109550                  0.198237              0.061786  \\\n",
       "1                12.629057                  0.680739              0.255785   \n",
       "2               193.386702                 11.709859              0.452508   \n",
       "3               208.624246                 12.743371              0.462084   \n",
       "\n",
       "   eval_metrics/mean_weighted_sum_quantile_loss        domain  num_variates  \n",
       "0                                      0.049082      Econ/Fin             1  \n",
       "1                                      0.198957  Web/CloudOps             7  \n",
       "2                                      0.356206  Web/CloudOps             7  \n",
       "3                                      0.368572  Web/CloudOps             7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"{results_filename}.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "dd-sharing": {
   "allowed_groups": [
    "subproduct-datascience",
    "combined-data-science",
    "team-largemodelfoundationsresearch",
    ""
   ],
   "allowed_users": [
    ""
   ],
   "retention_period": "90"
  },
  "kernelspec": {
   "display_name": "toto_test_env",
   "language": "python",
   "name": "toto-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
